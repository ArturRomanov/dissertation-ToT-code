{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT Evaluate Mistral Tuned\n",
    "\n",
    "This notebook evaluates tuned Mistral locally with base (Mistral AI, 2024) and a checkpoint from the ToT-tuning-1 notebook for the generation of answers and (Ollama, 2024a; Ollama, 2024b) for other functions based on test dataset produced from the ToT-data-answer-generator-and-checker notebook and based on the MMLU dataset (Hendrycks et al, 2021a; Hendrycks et al, 2021b; Hendrycks et al, 2023)\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    " \n",
    "Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "\n",
    "Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "\n",
    "Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code in Configure and Load sections in this notebook is in line with: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register at (Hugging Face, n.d.a), perform the access request at (Mistral AI, 2024), follow (Hugging Face, n.d.b) to insert your private secret in the below code instead of \"\" as indicated in (Hugging Face, n.d.b).\n",
    "\n",
    "Hugging Face, n.d.a. The AI community building the future [Online]. Available from: https://huggingface.co [Accessed 19 October 2024].\n",
    "\n",
    "Hugging Face, n.d.b. User access tokens [Online]. Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "private_secret = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "import os\n",
    "#\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code, that is, the value, reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm = AutoModelForCausalLM.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm = AutoTokenizer.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code is based on: Hugging Face, n.d. Accessing Private/Gated Models (v.3.0.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers.js/en/guides/private [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter and value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the parameter and value, are in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.pad_token = tuning_tokenizer_for_llm.unk_token\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.padding_side = \"right\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "first_extracted_index = \"input_ids\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "second_extracted_index = 1\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "extracted_value = tuning_tokenizer_for_llm(tuning_tokenizer_for_llm.unk_token)[first_extracted_index][second_extracted_index]\n",
    "tuning_llm.config.pad_token_id = extracted_value\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from peft import PeftModel\n",
    "#\n",
    "\n",
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "combined_llm = PeftModel.from_pretrained(tuning_llm, \"ToT-tuning-1/checkpoint-643\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset_path_csv = \"test-dataset-for-tuning/test_dataset_0_0_671.csv\"\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset = pd.read_csv(test_dataset_path_csv)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the evaluation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"string_answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"llm_answer_check\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"llm_answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToT evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tot_evaluation(enhanced_question, generated_tot_answer, letter_answer, letter_answer_text):\n",
    "    # Prompt (lines 1, 2, 5, 8, 9, 10 in the tot_evaluation_prompt) is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Prompt (line 1 in the tot_evaluation_prompt) reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    #\n",
    "    # Please note that enhanced_question variable in the tot_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_tot_answer variable in the tot_evaluation_prompt would include an answer based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_tot_answer variable in the tot_evaluation_prompt would include an answer from the tuned Mistral 7B Instruct version v0.3 at the period of running this notebook and which would be used locally using:\n",
    "    # Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "    # checkpoint from the ToT-tuning-1 notebook\n",
    "    # enhanced_prompt_to_generate_answer in generate_and_check_answers function in Generate and Evaluate Answers section of this notebook\n",
    "    # enhanced_question values for the prompt from test_dataset in the notebook which is based on the MMLU dataset:\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer variable in the tot_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer_text variable in the tot_evaluation_prompt would include data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    tot_evaluation_prompt = f'''\n",
    "    The question is: {enhanced_question}\n",
    "    The following text delimited by ####### is the answer to the question.\n",
    "\n",
    "    #######\n",
    "    {generated_tot_answer}\n",
    "    #######\n",
    "\n",
    "    Evaluate the text delimited by ####### which is the answer to the question and conclude if the answer provided at the bottom of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}.\n",
    "    If the answer provided at the bottom of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}, output correct.\n",
    "    Otherwise, if the answer provided at the bottom of the text delimited by ####### is not {letter_answer} and/or {letter_answer_text}, output incorrect.\n",
    "    You must output only one word: correct or incorrect.\n",
    "    '''\n",
    "    print(tot_evaluation_prompt)\n",
    "    return tot_evaluation_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Evaluate Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders to save generated and evaluated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "def create_data_for_tuning_folder_address(current_path, data_for_tuning_folder_name):\n",
    "    return current_path + \"/\" + data_for_tuning_folder_name\n",
    "\n",
    "def create_folder_for_data_for_tuning(data_for_tuning_folder_name, data_for_tuning_folder_address):\n",
    "    # Code, that is, the check for folder, adapted from: Python Software Foundation, 2024. os.path - Common pathname manipulations. os.path.exists (v.3.12.5) [Online]. \n",
    "    # Available from: https://docs.python.org/3/library/os.path.html#os.path.exists [Accessed 25 August 2024].\n",
    "    data_for_tuning_folder_is_present = os.path.exists(data_for_tuning_folder_address)\n",
    "    #\n",
    "    \n",
    "    if not data_for_tuning_folder_is_present:\n",
    "        os.mkdir(data_for_tuning_folder_address)\n",
    "        print(f'Created {data_for_tuning_folder_name} folder')\n",
    "    else:\n",
    "        print(f'{data_for_tuning_folder_name} folder is present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if answers were already generated and load the appropriate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_and_load_generated_answers_to_continue(initial_dataset, data_for_tuning_folder_name, data_for_tuning_folder_address):\n",
    "    '''\n",
    "    This function checks if there are any files with the generated answers in a folder and if so, loads intermediate files to continue generating answers or loads a completed file, \n",
    "    otherwise uses the intial dataset to start generating answers from the beginning\n",
    "    '''\n",
    "    # Code, that is, the finding of files for tuning, adapted from: Python Software Foundation, 2024. os - Miscellaneous operating system interfaces. os.listdir (v.3.12.5) [Online]. \n",
    "    # Available from: https://docs.python.org/3/library/os.html#os.listdir [Accessed 31 August 2024].\n",
    "    files_for_tuning = os.listdir(data_for_tuning_folder_address)\n",
    "    #\n",
    "\n",
    "    if len(files_for_tuning):\n",
    "        # Code, that is, the function and the ordering of files for tuning, adapted from: Python Software Foundation, 2024. Built-in Types. sort (v.3.12.5) [Online]. \n",
    "        # Available from: https://docs.python.org/3/library/stdtypes.html#list.sort [Accessed 31 August 2024].\n",
    "        def check_version_of_file(file):\n",
    "            return int(file.split('_')[2])\n",
    "        \n",
    "        files_for_tuning.sort(key=check_version_of_file, reverse=True)\n",
    "        #\n",
    "        print(f\"The following files were found in the {data_for_tuning_folder_name} folder. {files_for_tuning}\")\n",
    "        recent_file_for_tuning = files_for_tuning[0]\n",
    "        print(f\"The most recent file in the {data_for_tuning_folder_name} folder is {recent_file_for_tuning}\")\n",
    "        recent_file_for_tuning_attributes = recent_file_for_tuning.split('_')\n",
    "\n",
    "        recent_file_for_tuning_address = data_for_tuning_folder_address + '/' + recent_file_for_tuning\n",
    "        # Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "        dataset_for_generated_answers = pd.read_csv(recent_file_for_tuning_address)\n",
    "        #\n",
    "        dataset_for_generated_answers_version = int(recent_file_for_tuning_attributes[2])\n",
    "        generated_answers_count = int(recent_file_for_tuning_attributes[3])\n",
    "        total_dataset_rows = int(recent_file_for_tuning_attributes[4].split('.')[0])\n",
    "        \n",
    "        if generated_answers_count < total_dataset_rows:\n",
    "            print(f\"The {data_for_tuning_folder_name} folder has intermediate files with generated answers. Continue the generation of answers. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        elif generated_answers_count == total_dataset_rows:\n",
    "            print(f\"The {data_for_tuning_folder_name} folder has already generated answers. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        return dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows\n",
    "    else:\n",
    "        dataset_for_generated_answers = initial_dataset\n",
    "        dataset_for_generated_answers_version = 0\n",
    "        generated_answers_count = 0\n",
    "        total_dataset_rows = len(dataset_for_generated_answers)\n",
    "        print(f\"The {data_for_tuning_folder_name} folder does not have files with generated answers. Start the generation of answers from the beginning. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        return dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and evaluate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the import, reused from: LangChain, 2023. OllamaLLM [Online]. \n",
    "# Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "from langchain_ollama import OllamaLLM\n",
    "#\n",
    "# Code, that is, the import, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "# Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024].\n",
    "import numpy as np\n",
    "#\n",
    "\n",
    "# Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "from transformers import pipeline\n",
    "#\n",
    "\n",
    "def generate_and_check_answers(data_for_tuning_folder_name, data_for_tuning_folder_address, dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows, tot):\n",
    "    '''\n",
    "    This function generates answers in the simple or ToT style to be used for evaluation, checks the answers for quality, saves the answers periodically with the intermediate versions\n",
    "    '''\n",
    "    if generated_answers_count < total_dataset_rows:\n",
    "        generated_answers_count_updated = generated_answers_count\n",
    "        dataset_for_generated_answers_version_updated = dataset_for_generated_answers_version\n",
    "        save_dataset_count = 15\n",
    "        # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "        for dataset_record in dataset_for_generated_answers.itertuples():\n",
    "        #\n",
    "            if (dataset_record.generated_answer is None or\n",
    "            # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "            # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "            dataset_record.generated_answer is np.nan or\n",
    "            #\n",
    "            # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "            pd.isna(dataset_record.generated_answer) == True):\n",
    "            #\n",
    "                retries_for_quality = 0\n",
    "                max_retries_for_quality = 3\n",
    "                while retries_for_quality < max_retries_for_quality:\n",
    "                    # Code, that is, the value, reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "                    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "                    # (mrspiggot, 2023, line 23)\n",
    "                    element_before_ask = \"The question is: \"\n",
    "                    #\n",
    "                    # Code, that is, the value, is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "                    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "                    # (mrspiggot, 2023, lines 25)\n",
    "                    # Code, that is, the value, is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                    element_after_ask = \"What is the answer and the answer letter?\"\n",
    "                    #\n",
    "                    # Code, that is, the parameter, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "                    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "                    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "                    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "                    # Code, that is, the value, is based on: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "                    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "                    # Code, that is, the value, is based on transformations in ToT-data-ETL notebook to data from and according to: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                    # Code, that is, the first, second, fifth, seventh elements of value, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "                    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, first, second and seventh strings, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    enhanced_prompt_to_generate_answer = \"<s>\" + \"[INST] \" + element_before_ask + dataset_record.enhanced_question + \"\\n\" + element_after_ask + \" [/INST] \"\n",
    "                    #\n",
    "                    # Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    generated_answer_type = \"text-generation\"\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_generator = pipeline(model = combined_llm, \n",
    "                    tokenizer = tuning_tokenizer_for_llm, \n",
    "                    task = generated_answer_type,\n",
    "                    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    device = \"mps\",\n",
    "                    #\n",
    "                    # Code is based on: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code is based on: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, adapted from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "                    max_new_tokens = 5000\n",
    "                    #\n",
    "                    )\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_first_index = 0\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_second_index = \"generated_text\"\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    generated_answer_with_enhanced_prompt = answer_generator(enhanced_prompt_to_generate_answer)[answer_first_index][answer_second_index]\n",
    "                    print(generated_answer_with_enhanced_prompt)\n",
    "                    #\n",
    "                    enhanced_prompt_to_generate_answer_part = enhanced_prompt_to_generate_answer[(len(enhanced_prompt_to_generate_answer)-50):len(enhanced_prompt_to_generate_answer)]\n",
    "                    print(enhanced_prompt_to_generate_answer_part)\n",
    "                    generated_answer = generated_answer_with_enhanced_prompt.split(enhanced_prompt_to_generate_answer_part)[1]\n",
    "                    print(generated_answer)\n",
    "                    # This simple check for quality checks that the generated answer has at least different second level thoughts based on the\n",
    "                    # Zhang, Z., Ye, Z., Shen, Y. and Gan, C., 2023. Autonomous Tree-Search Ability of Large Language Models. Ithaca: Cornell University Library, arXiv.org. arXiv [Online]. Available from: https://arxiv.org/pdf/2310.10686.pdf [Accessed 25 August 2024].\n",
    "                    # Page 13, C.1\n",
    "                    if ((\"1.1\" in generated_answer and \n",
    "                    \"1.2\" in generated_answer and \n",
    "                    \"2.1\" in generated_answer and \n",
    "                    \"2.2\" in generated_answer and \n",
    "                    \"3.1\" in generated_answer and \n",
    "                    \"3.2\" in generated_answer) and \n",
    "                    #\n",
    "                    # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "                    # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "                    # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "                    # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "                    # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "                    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "                    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "                    # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "                    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "                    # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "                    # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "                    # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "                    # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "                    # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                    (f\"Answer: A\" in generated_answer or\n",
    "                        f\"Answer: B\" in generated_answer or \n",
    "                        f\"Answer: C\" in generated_answer or  \n",
    "                        f\"Answer: D\" in generated_answer or\n",
    "                        f\"answer: A\" in generated_answer or\n",
    "                        f\"answer: B\" in generated_answer or\n",
    "                        f\"answer: C\" in generated_answer or\n",
    "                        f\"answer: D\" in generated_answer or\n",
    "                        f\"Answer is A\" in generated_answer or\n",
    "                        f\"Answer is B\" in generated_answer or\n",
    "                        f\"Answer is C\" in generated_answer or\n",
    "                        f\"Answer is D\" in generated_answer or\n",
    "                        f\"answer is A\" in generated_answer or\n",
    "                        f\"answer is B\" in generated_answer or\n",
    "                        f\"answer is C\" in generated_answer or\n",
    "                        f\"answer is D\" in generated_answer or\n",
    "                        f\"Answer is: A\" in generated_answer or\n",
    "                        f\"Answer is: B\" in generated_answer or\n",
    "                        f\"Answer is: C\" in generated_answer or\n",
    "                        f\"Answer is: D\" in generated_answer or\n",
    "                        f\"answer is: A\" in generated_answer or\n",
    "                        f\"answer is: B\" in generated_answer or\n",
    "                        f\"answer is: C\" in generated_answer or\n",
    "                        f\"answer is: D\" in generated_answer)):\n",
    "                    #\n",
    "                        retries_for_quality = max_retries_for_quality\n",
    "                    retries_for_quality += 1\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_generated_answers.loc[dataset_for_generated_answers['id'] == dataset_record.id, \"generated_answer\"] = generated_answer\n",
    "                #\n",
    "                generated_answers_count_updated += 1\n",
    "                print(f\"The generated answer for {dataset_record.id} was generated with quality check and {retries_for_quality} retries\")\n",
    "                if (generated_answers_count_updated % save_dataset_count == 0) or (generated_answers_count_updated == total_dataset_rows):\n",
    "                    dataset_for_generated_answers_version_updated += 1\n",
    "                    dataset_type = data_for_tuning_folder_name.split('-')[0]\n",
    "                    dataset_for_generated_answers_address = data_for_tuning_folder_address + '/' + f'{dataset_type}_dataset_{dataset_for_generated_answers_version_updated}_{generated_answers_count_updated}_{total_dataset_rows}.csv'\n",
    "                    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "                    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "                    dataset_for_generated_answers.to_csv(dataset_for_generated_answers_address, index=False)\n",
    "                    #\n",
    "                    print(f\"The dataset for generated answers was saved for the version {dataset_for_generated_answers_version_updated}. The count of generated answers is {generated_answers_count_updated}. The count of total rows is {total_dataset_rows}\")\n",
    "            else:\n",
    "                print(f\"The generated answer for id {dataset_record.id} was already generated\")\n",
    "    elif generated_answers_count == total_dataset_rows:\n",
    "        print(\"The generated answers were already generated for the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the import, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "# Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024].\n",
    "import numpy as np\n",
    "#\n",
    "\n",
    "# Code, that is, the import, reused from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "# Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "import re\n",
    "#\n",
    "\n",
    "def string_answer_evaluate(data_for_tuning_folder_address, dataset_for_string_answer_evaluation_name, dataset_for_string_answer_evaluation, extract_first_answer):\n",
    "    '''\n",
    "    This function evaluates generated answers for the correct answer based on the string pattern of answers where 1 is correct answer and 0 is incorrect answer\n",
    "    '''\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in dataset_for_string_answer_evaluation.itertuples():\n",
    "    #\n",
    "        if (dataset_record.string_answer_evaluation is None or\n",
    "        # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "        # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "        dataset_record.string_answer_evaluation is np.nan or\n",
    "        #\n",
    "        # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "        pd.isna(dataset_record.string_answer_evaluation) == True):\n",
    "        #\n",
    "            if extract_first_answer:\n",
    "                print(f\"Extract first answer is used for id {dataset_record.id}\")\n",
    "                # Code is adapted from and based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "                # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "                # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "                # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "                # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "                # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "                # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "                # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "                # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "                # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "                # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "                # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "                # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "                # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                string_answer_to_extract = re.compile(\"(Answer: |answer: |Answer is |answer is |Answer is: |answer is: )[A-D]\")\n",
    "                #\n",
    "                # Code is adapted from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                string_answer_check = string_answer_to_extract.search(dataset_record.generated_answer)\n",
    "                #\n",
    "                # Code is adapted from and based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                if string_answer_check is not None:\n",
    "                #\n",
    "                    print(f\"String answer check for id {dataset_record.id} is {string_answer_check}\")\n",
    "                    # Code is adapted from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                    # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                    string_answer_indices = string_answer_check.span()\n",
    "                    #\n",
    "                    # Code is based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                    # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                    generated_answer = dataset_record.generated_answer[0:string_answer_indices[1]]\n",
    "                    #\n",
    "                else:\n",
    "                    generated_answer = dataset_record.generated_answer\n",
    "            else:\n",
    "                generated_answer = dataset_record.generated_answer\n",
    "\n",
    "            print(f\"The generated answer to be checked for id {dataset_record.id} is {generated_answer}\")\n",
    "\n",
    "            # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "            # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "            # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "            # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "            # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "            # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "            # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "            # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "            # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            if (f\"Answer: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"Answer is {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer is {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"Answer is: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer is: {dataset_record.letter_answer}\" in generated_answer):\n",
    "            #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = 1\n",
    "                #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = 1\n",
    "                #\n",
    "            # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "            # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "            # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "            # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "            # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "            # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "            # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "            # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "            # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            elif (f\"Answer: A\" in generated_answer or\n",
    "                f\"Answer: B\" in generated_answer or \n",
    "                f\"Answer: C\" in generated_answer or  \n",
    "                f\"Answer: D\" in generated_answer or\n",
    "                f\"answer: A\" in generated_answer or\n",
    "                f\"answer: B\" in generated_answer or\n",
    "                f\"answer: C\" in generated_answer or\n",
    "                f\"answer: D\" in generated_answer or\n",
    "                f\"Answer is A\" in generated_answer or\n",
    "                f\"Answer is B\" in generated_answer or\n",
    "                f\"Answer is C\" in generated_answer or\n",
    "                f\"Answer is D\" in generated_answer or\n",
    "                f\"answer is A\" in generated_answer or\n",
    "                f\"answer is B\" in generated_answer or\n",
    "                f\"answer is C\" in generated_answer or\n",
    "                f\"answer is D\" in generated_answer or\n",
    "                f\"Answer is: A\" in generated_answer or\n",
    "                f\"Answer is: B\" in generated_answer or\n",
    "                f\"Answer is: C\" in generated_answer or\n",
    "                f\"Answer is: D\" in generated_answer or\n",
    "                f\"answer is: A\" in generated_answer or\n",
    "                f\"answer is: B\" in generated_answer or\n",
    "                f\"answer is: C\" in generated_answer or\n",
    "                f\"answer is: D\" in generated_answer):\n",
    "            #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = 0\n",
    "                #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = 0\n",
    "                #\n",
    "            else:\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = None\n",
    "                #\n",
    "        else:\n",
    "            print(f\"The string answer evaluation for id {dataset_record.id} was already generated\")\n",
    "    if extract_first_answer:\n",
    "        dataset_for_string_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_string_answer_evaluation_name}_string_answer_evaluated_extracted.csv'\n",
    "    else:\n",
    "        dataset_for_string_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_string_answer_evaluation_name}_string_answer_evaluated.csv'\n",
    "    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "    dataset_for_string_answer_evaluation.to_csv(dataset_for_string_answer_evaluation_address, index=False)\n",
    "    #\n",
    "    if extract_first_answer:\n",
    "        print(f\"The dataset for string answer evaluation was saved to {dataset_for_string_answer_evaluation_name}_string_answer_evaluated_extracted.csv\")\n",
    "    else:\n",
    "        print(f\"The dataset for string answer evaluation was saved to {dataset_for_string_answer_evaluation_name}_string_answer_evaluated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer_evaluate(data_for_tuning_folder_address, dataset_for_llm_answer_evaluation_name, dataset_for_llm_answer_evaluation, tot):\n",
    "    '''\n",
    "    This fuction evaluates generated answers for the correct answer based on the Large Language Model which could not be evaluated with pattern of string answers \n",
    "    where 1 is correct answer and 0 is incorrect answer\n",
    "    '''\n",
    "    # Code, that is, the answer generator, adapted from: LangChain, 2023. OllamaLLM [Online]. \n",
    "    # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "    answer_generator_llm = OllamaLLM(\n",
    "        # Code, that is, the parameter, reused from: LangChain, 2023. OllamaLLM [Online]. \n",
    "        # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "        # Code, that is, the value, reused from: Ollama, 2024. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "        model=\"mixtral:8x7b-instruct-v0.1-fp16\"\n",
    "        #\n",
    "        )\n",
    "    #\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in dataset_for_llm_answer_evaluation.itertuples():\n",
    "    #\n",
    "        if (dataset_record.string_answer_evaluation is None or\n",
    "        # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "        # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "        dataset_record.string_answer_evaluation is np.nan or\n",
    "        #\n",
    "        # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "        pd.isna(dataset_record.string_answer_evaluation) == True):\n",
    "        #\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            if dataset_record.letter_answer == \"A\":\n",
    "                letter_answer_text = dataset_record.first_choice\n",
    "            elif dataset_record.letter_answer == \"B\":\n",
    "                letter_answer_text = dataset_record.second_choice\n",
    "            elif dataset_record.letter_answer == \"C\":\n",
    "                letter_answer_text = dataset_record.third_choice\n",
    "            elif dataset_record.letter_answer == \"D\":\n",
    "                letter_answer_text = dataset_record.fourth_choice\n",
    "            #\n",
    "\n",
    "            prompt_to_evaluate_answer = prompt_tot_evaluation(dataset_record.enhanced_question, dataset_record.generated_answer, dataset_record.letter_answer, letter_answer_text)\n",
    "\n",
    "            # Code, that is, the answer generation, adapted from: LangChain, 2023. OllamaLLM [Online]. \n",
    "            # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "            llm_checked_answer = answer_generator_llm.invoke(prompt_to_evaluate_answer)\n",
    "            #\n",
    "            print(llm_checked_answer)\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"llm_answer_check\"] = llm_checked_answer\n",
    "            #\n",
    "            if \"correct\" in llm_checked_answer.lower() and \"incorrect\" not in llm_checked_answer.lower() and \"not correct\" not in llm_checked_answer.lower():\n",
    "                llm_evaluated_answer = 1\n",
    "            else:\n",
    "                llm_evaluated_answer = 0\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"llm_answer_evaluation\"] = llm_evaluated_answer\n",
    "            #\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = llm_evaluated_answer\n",
    "            #\n",
    "        else:\n",
    "            print(f\"The string answer evaluation for id {dataset_record.id} was generated\")\n",
    "    dataset_for_llm_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_llm_answer_evaluation_name}_answer_evaluated.csv'\n",
    "    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "    dataset_for_llm_answer_evaluation.to_csv(dataset_for_llm_answer_evaluation_address, index=False)\n",
    "    #\n",
    "    print(f\"The dataset for llm answer evaluation was saved to {dataset_for_llm_answer_evaluation_name}_answer_evaluated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataset_path_for_answer_evaluation, dataset_to_evaluate, column_to_evaluate):\n",
    "    '''\n",
    "    This function evaluates the accuracy of results in a particular column in a dataset\n",
    "    '''\n",
    "    # Code adapted from: pandas, 2024. pandas.DataFrame.sum (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html#pandas-dataframe-sum [Accessed 13 October 2024].\n",
    "    dataset_to_evaluate_column_ones = dataset_to_evaluate[f'{column_to_evaluate}'].sum()\n",
    "    #\n",
    "    # Code adapted from: pandas, 2024. pandas.DataFrame.count (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.count.html#pandas-dataframe-count [Accessed 13 October 2024].\n",
    "    dataset_to_evaluate_column_numbers = dataset_to_evaluate[f'{column_to_evaluate}'].count()\n",
    "    #\n",
    "    # The equation is according to: Dutt, S., Chandramouli, S. and Das, A.K., 2019. Machine Learning [Online]. 1st ed. Uttar Pradesh, India: Pearson India. \n",
    "    # Available from: https://learning.oreilly.com/library/view/machine-learning-1st/9789353067373/ [Accessed 13 October 2024].\n",
    "    # Page 76\n",
    "    dataset_column_accuracy = dataset_to_evaluate_column_ones / dataset_to_evaluate_column_numbers\n",
    "    #\n",
    "    print(f'The dataset to evaluate column ones of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_to_evaluate_column_ones}')\n",
    "    print(f'The dataset to evaluate column numbers of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_to_evaluate_column_numbers}')\n",
    "    print(f'The dataset column accuracy of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_column_accuracy}')\n",
    "    return dataset_column_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral ToT tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first answer in string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = True\n",
    "extract_first_answer = True\n",
    "test_data_for_evaluation_folder_name = \"test-dataset-for-evaluation-mistral-tot-tuned\"\n",
    "test_data_for_evaluation_folder_address = create_data_for_tuning_folder_address(current_path, test_data_for_evaluation_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_for_data_for_tuning(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows = check_and_load_generated_answers_to_continue(test_dataset, test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "generate_and_check_answers(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address, test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not extract first answer in string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_first_answer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
