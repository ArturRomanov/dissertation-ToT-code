{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fine-tunes Mistral locally with (Mistral AI, 2024) based on the train dataset and validation dataset produced from the ToT-data-answer-generator-and-checker notebook using (Ollama, 2024a; Ollama, 2024b) and based on the MMLU dataset (Hendrycks et al, 2021a; Hendrycks et al, 2021b; Hendrycks et al, 2023).\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    " \n",
    "Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "\n",
    "Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "\n",
    "Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register at (Hugging Face, n.d.a), perform the access request at (Mistral AI, 2024), follow (Hugging Face, n.d.b) to insert your private secret in the below code instead of \"\" as indicated in (Hugging Face, n.d.b).\n",
    "\n",
    "Hugging Face, n.d.a. The AI community building the future [Online]. Available from: https://huggingface.co [Accessed 19 October 2024].\n",
    "\n",
    "Hugging Face, n.d.b. User access tokens [Online]. Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "private_secret = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code, that is, the value, reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "tuning_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "tuning_llm = AutoModelForCausalLM.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "tuning_tokenizer_for_llm = AutoTokenizer.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code is based on: Hugging Face, n.d. Accessing Private/Gated Models (v.3.0.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers.js/en/guides/private [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dataset_path_csv = \"train-dataset-for-tuning/train_dataset_217_3245_3245.csv\"\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "train_dataset = pd.read_csv(train_dataset_path_csv)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_path_csv = \"validation-dataset-for-tuning/validation_dataset_38_557_557.csv\"\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "validation_dataset = pd.read_csv(validation_dataset_path_csv)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter and value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "tuning_tokenizer_for_llm.pad_token = tuning_tokenizer_for_llm.unk_token\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "tuning_tokenizer_for_llm.padding_side = \"right\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "first_extracted_index = \"input_ids\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "second_extracted_index = 1\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "extracted_value = tuning_tokenizer_for_llm(tuning_tokenizer_for_llm.unk_token)[first_extracted_index][second_extracted_index]\n",
    "tuning_llm.config.pad_token_id = extracted_value\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "tuning_llm.config.use_cache = False\n",
    "#\n",
    "\n",
    "# Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code adapted from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Using Accelerate (v.4.46.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#using--accelerate [Accessed 26 October 2024].\n",
    "tuning_llm.gradient_checkpointing_enable()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "#\n",
    "\n",
    "# Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "tuning_llm = prepare_model_for_kbit_training(tuning_llm)\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "settings_for_tuning_llm = LoraConfig(\n",
    "    # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the parameter reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    # Code, that is, the value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    lora_alpha = 16,\n",
    "    #\n",
    "    # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    lora_dropout = 0.1,\n",
    "    #\n",
    "    # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the parameter reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    # Code, that is, the value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    r = 64,\n",
    "    #\n",
    "    # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    bias = \"none\",\n",
    "    #\n",
    "    # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the parameter reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    # Code, that is, the value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    #\n",
    "    # Code, that is, the parameter and the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "    # Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "    # Code, that is, the parameter and the value of the first five elements, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    #\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "tuning_llm = get_peft_model(tuning_llm, settings_for_tuning_llm)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "train_dataset.loc[:, \"answer_and_structure\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "validation_dataset.loc[:, \"answer_and_structure\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_answer_and_structure(generated_answers_dataset):\n",
    "    '''\n",
    "    This function filters generated answers for the records with the correct structure and answer\n",
    "    '''\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in generated_answers_dataset.itertuples():\n",
    "    #\n",
    "        # This simple check for quality checks that the generated answer has at least different second level thoughts based on the\n",
    "        # Zhang, Z., Ye, Z., Shen, Y. and Gan, C., 2023. Autonomous Tree-Search Ability of Large Language Models. Ithaca: Cornell University Library, arXiv.org. arXiv [Online]. Available from: https://arxiv.org/pdf/2310.10686.pdf [Accessed 25 August 2024].\n",
    "        # Page 13, C.1\n",
    "        if ((\"1.1\" in dataset_record.generated_answer and \n",
    "        \"1.2\" in dataset_record.generated_answer and \n",
    "        \"2.1\" in dataset_record.generated_answer and \n",
    "        \"2.2\" in dataset_record.generated_answer and \n",
    "        \"3.1\" in dataset_record.generated_answer and \n",
    "        \"3.2\" in dataset_record.generated_answer) and \n",
    "        #\n",
    "        # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "        # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "        # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "        # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "        # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "        # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "        # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "        # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "        # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "        # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "        (f\"Answer: {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)] or\n",
    "        f\"answer: {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)] or\n",
    "        f\"Answer is {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)] or\n",
    "        f\"answer is {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)] or\n",
    "        f\"Answer is: {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)] or\n",
    "        f\"answer is: {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-500:len(dataset_record.generated_answer)])):\n",
    "        #\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            generated_answers_dataset.loc[generated_answers_dataset['id'] == dataset_record.id, \"answer_and_structure\"] = True\n",
    "            #\n",
    "        else:\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            generated_answers_dataset.loc[generated_answers_dataset['id'] == dataset_record.id, \"answer_and_structure\"] = False\n",
    "            #\n",
    "    # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 1 November 2024].\n",
    "    generated_answers_dataset_filtered_for_answer_and_structure = generated_answers_dataset.loc[generated_answers_dataset[\"answer_and_structure\"] == True]\n",
    "    #\n",
    "    return generated_answers_dataset_filtered_for_answer_and_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_filtered_for_answer_and_structure = filter_for_answer_and_structure(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_filtered_for_answer_and_structure = filter_for_answer_and_structure(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_answers_to_be_consistent(generated_answers_dataset):\n",
    "    '''\n",
    "    This function adjusts generated answers to be more consistent\n",
    "    '''\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in generated_answers_dataset.itertuples():\n",
    "    #\n",
    "        # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "        # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "        # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "        # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "        # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "        # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "        # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "        # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "        # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "        # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "        if (f\"Answer: {dataset_record.letter_answer}\" in dataset_record.generated_answer[len(dataset_record.generated_answer)-15:len(dataset_record.generated_answer)]):\n",
    "        #\n",
    "            print(f\"The generated answer for {dataset_record.id} has a consistent answer\")\n",
    "        else:\n",
    "            # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "            # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "            # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "            # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "            # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "            # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "            # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "            # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "            # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            generated_answer_consistent = dataset_record.generated_answer + \"\\n\\n\" + f\"Answer: {dataset_record.letter_answer}\"\n",
    "            #\n",
    "            \n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            generated_answers_dataset.loc[generated_answers_dataset['id'] == dataset_record.id, \"generated_answer\"] = generated_answer_consistent\n",
    "            #\n",
    "\n",
    "            print(f\"The consistent answer was added to generated answer for {dataset_record.id}\")\n",
    "    generated_answers_dataset_with_consistent_answers = generated_answers_dataset\n",
    "    return generated_answers_dataset_with_consistent_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_consistent = adjust_answers_to_be_consistent(train_dataset_filtered_for_answer_and_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_consistent = adjust_answers_to_be_consistent(validation_dataset_filtered_for_answer_and_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "def create_data_for_tuning_folder_address(current_path, data_for_tuning_folder_name):\n",
    "    return current_path + \"/\" + data_for_tuning_folder_name\n",
    "\n",
    "def create_folder_for_data_for_tuning(data_for_tuning_folder_name, data_for_tuning_folder_address):\n",
    "    # Code, that is, the check for folder, adapted from: Python Software Foundation, 2024. os.path - Common pathname manipulations. os.path.exists (v.3.12.5) [Online]. \n",
    "    # Available from: https://docs.python.org/3/library/os.path.html#os.path.exists [Accessed 25 August 2024].\n",
    "    data_for_tuning_folder_is_present = os.path.exists(data_for_tuning_folder_address)\n",
    "    #\n",
    "    \n",
    "    if not data_for_tuning_folder_is_present:\n",
    "        os.mkdir(data_for_tuning_folder_address)\n",
    "        print(f'Created {data_for_tuning_folder_name} folder')\n",
    "    else:\n",
    "        print(f'{data_for_tuning_folder_name} folder is present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_tuning_final_folder_name = \"train-dataset-for-tuning-final-1\"\n",
    "train_data_for_tuning_final_folder_address = create_data_for_tuning_folder_address(current_path, train_data_for_tuning_final_folder_name)\n",
    "create_folder_for_data_for_tuning(train_data_for_tuning_final_folder_name, train_data_for_tuning_final_folder_address)\n",
    "\n",
    "train_dataset_final_for_tuning_name = \"train_dataset_final\"\n",
    "train_dataset_final_path_for_tuning_address = f\"{train_data_for_tuning_final_folder_address}/{train_dataset_final_for_tuning_name}.csv\"\n",
    "print(f'Train dataset final path for tuning address is {train_dataset_final_path_for_tuning_address}')\n",
    "\n",
    "# Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "train_dataset_consistent.to_csv(train_dataset_final_path_for_tuning_address, index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_for_tuning_final_folder_name = \"validation-dataset-for-tuning-final-1\"\n",
    "validation_data_for_tuning_final_folder_address = create_data_for_tuning_folder_address(current_path, validation_data_for_tuning_final_folder_name)\n",
    "create_folder_for_data_for_tuning(validation_data_for_tuning_final_folder_name, validation_data_for_tuning_final_folder_address)\n",
    "\n",
    "validation_dataset_final_for_tuning_name = \"validation_dataset_final\"\n",
    "validation_dataset_final_path_for_tuning_address = f\"{validation_data_for_tuning_final_folder_address}/{validation_dataset_final_for_tuning_name}.csv\"\n",
    "print(f'Validation dataset final path for tuning address is {validation_dataset_final_path_for_tuning_address}')\n",
    "\n",
    "# Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "validation_dataset_consistent.to_csv(validation_dataset_final_path_for_tuning_address, index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "def prepare_data_for_tuning(record):\n",
    "    # Code, that is, the value, reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    element_before_ask = \"The question is: \"\n",
    "    #\n",
    "    # Code, that is, the value, is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 25)\n",
    "    # Code, that is, the value, is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    element_after_ask = \"What is the answer and the answer letter?\"\n",
    "    #\n",
    "    # Code, that is, the parameter, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is based on: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is based on transformations in ToT-data-ETL notebook to data from and according to: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    # Code, that is, the first, second, fifth, seventh, ninth elements of value, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    generated_answer_for_tuning = \"<s>\" + \"[INST] \" + element_before_ask + record[\"enhanced_question\"] + \"\\n\" + element_after_ask + \" [/INST] \" + record[\"generated_answer\"] + \"</s>\"\n",
    "    #\n",
    "    # Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is the value, is based on and adapted from: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "    prepared_data_for_tuning = tuning_tokenizer_for_llm(\n",
    "        # Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code is adapted from: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "        generated_answer_for_tuning,\n",
    "        #\n",
    "        # Code, that is, the parameter, is reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the parameter, is reused from: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, is adapted from: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, is based on: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, is based on: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "        # Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "        # Code, that is, the value, is based on: Mistral AI, 2023. Model Card for Mistral-7B-Instruct-v0.2 [Online]. s.l.: Hugging Face. \n",
    "        # Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, is based on: Mozilla, 2024. Mistral 7B Instruct v0.3 - llamafile [Online]. s.l.: Hugging Face.\n",
    "        # Availabel from: https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.3-llamafile [Accessed 2 November 2024].\n",
    "        max_length = 31000,\n",
    "        #\n",
    "        # Code is reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code is reused from: Hugging Face, n.d. Padding and truncation (v4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/pad_truncation [Accessed 2 November 2024].\n",
    "        truncation = True\n",
    "        #\n",
    "    )\n",
    "    #\n",
    "    # Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    parameter_index = \"labels\"\n",
    "    #\n",
    "    # Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    value_index = \"input_ids\"\n",
    "    #\n",
    "    # Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    prepared_data_for_tuning[parameter_index] = prepared_data_for_tuning[value_index].copy()\n",
    "    return prepared_data_for_tuning\n",
    "    #\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the import, reused from: Hugging Face, n.d. Loading methods. datasets.load_dataset (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset [Accessed 3 November 2024].\n",
    "from datasets import load_dataset\n",
    "#\n",
    "\n",
    "# Code adapted from: Hugging Face, n.d. Loading methods. datasets.load_dataset (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset [Accessed 3 November 2024].\n",
    "# Code, that is, the value reused from: Hugging Face, n.d. Loading methods. datasets.load_dataset (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset [Accessed 3 November 2024].\n",
    "dataset_final_type = \"csv\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Hugging Face, n.d. Loading methods. datasets.load_dataset (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset [Accessed 3 November 2024].\n",
    "train_dataset_final = load_dataset(dataset_final_type, data_files = train_dataset_final_path_for_tuning_address)\n",
    "#\n",
    "\n",
    "# Code adapted from: Hugging Face, n.d. Loading methods. datasets.load_dataset (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset [Accessed 3 November 2024].\n",
    "validation_dataset_final = load_dataset(dataset_final_type, data_files = validation_dataset_final_path_for_tuning_address)\n",
    "#\n",
    "\n",
    "# Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "prepared_train_dataset = train_dataset_final.map(prepare_data_for_tuning)\n",
    "#\n",
    "\n",
    "# Code is adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "prepared_validation_dataset = validation_dataset_final.map(prepare_data_for_tuning)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "# Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "import matplotlib.pyplot as graph_to_present\n",
    "\n",
    "def create_graph(prepared_data_for_tuning):\n",
    "    values = []\n",
    "    for value in prepared_data_for_tuning:\n",
    "        values.append(len(value[\"input_ids\"]))\n",
    "\n",
    "    graph_to_present.figure(figsize=(20, 12))\n",
    "    graph_to_present.hist(values, bins = 100)\n",
    "    graph_to_present.title(\"Allocation of tokens\")\n",
    "    graph_to_present.ylabel(\"Repetitiveness\")\n",
    "    graph_to_present.xlabel(\"Tokens\")\n",
    "    graph_to_present.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "# Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "create_graph(prepared_train_dataset[\"train\"])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "# Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "create_graph(prepared_validation_dataset[\"train\"])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. Main classes. Dataset. filter (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/v3.1.0/package_reference/main_classes#datasets.Dataset.filter [Accessed 3 November 2024].\n",
    "def adjust_prepared_dataset(record):\n",
    "    # Code adapted from: Hugging Face, n.d. Main classes. Dataset. filter (V3.1.0) [Online]. \n",
    "    # Available from: https://huggingface.co/docs/datasets/v3.1.0/package_reference/main_classes#datasets.Dataset.filter [Accessed 3 November 2024].\n",
    "    # Code adapted from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "    # Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "    # Code, that is, the value reused from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "    # Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "    record_element = \"input_ids\"\n",
    "    #\n",
    "    # Code adapted from: Hugging Face, n.d. Main classes. Dataset. filter (V3.1.0) [Online]. \n",
    "    # Available from: https://huggingface.co/docs/datasets/v3.1.0/package_reference/main_classes#datasets.Dataset.filter [Accessed 3 November 2024].\n",
    "    # Code adapted from: brevdev, 2024. Fine-tuning Mistral on your own data (mistral-finetune-own-data.ipynb) [computer program].\n",
    "    # Available from: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb [Accessed 3 November 2024].\n",
    "    return len(record[record_element]) <= 4000\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. Main classes. Dataset. filter (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/v3.1.0/package_reference/main_classes#datasets.Dataset.filter [Accessed 3 November 2024].\n",
    "prepared_train_dataset_final = prepared_train_dataset.filter(adjust_prepared_dataset)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. Main classes. Dataset. filter (V3.1.0) [Online]. \n",
    "# Available from: https://huggingface.co/docs/datasets/v3.1.0/package_reference/main_classes#datasets.Dataset.filter [Accessed 3 November 2024].\n",
    "prepared_validation_dataset_final = prepared_validation_dataset.filter(adjust_prepared_dataset)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "import transformers\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "# Code, that is, the value, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "tuner = transformers.Trainer(\n",
    "    # Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(\n",
    "        # Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        tuning_tokenizer_for_llm,\n",
    "        #\n",
    "        # Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        mlm = False\n",
    "        #\n",
    "    ),\n",
    "    #\n",
    "    # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    train_dataset = prepared_train_dataset_final[\"train\"],\n",
    "    #\n",
    "    # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    eval_dataset = prepared_validation_dataset_final[\"train\"],\n",
    "    #\n",
    "    # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    model = tuning_llm,\n",
    "    #\n",
    "    # Code, that is, the parameter and the value, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    args = transformers.TrainingArguments(\n",
    "        # Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        warmup_steps = 5,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        output_dir = \"ToT-tuning-1\",\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # The value is reused from: Zhang, Z., Ye, Z., Shen, Y. and Gan, C., 2023. Autonomous Tree-Search Ability of Large Language Models. \n",
    "        # Ithaca: Cornell University Library, arXiv.org. arXiv [Online]. Available from: https://arxiv.org/pdf/2310.10686.pdf [Accessed 9 November 2024].\n",
    "        # Page 13\n",
    "        learning_rate = 0.00001,\n",
    "        #\n",
    "        # Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        gradient_checkpointing = True,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, tha value, is based on: Hugging Face, n.d. Methods and tools for efficient training on a single GPU (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#methods-and-tools-for-efficient-training-on-a-single-gpu [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, is based on: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        # The value reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        # Para.1\n",
    "        # Code reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        per_device_train_batch_size = 1,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, adapted from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, tha value, is based on: Hugging Face, n.d. Methods and tools for efficient training on a single GPU (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#methods-and-tools-for-efficient-training-on-a-single-gpu [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, is based on: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        # The value reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        # Para.1\n",
    "        # Code, that is, the value, reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        per_device_eval_batch_size = 1,\n",
    "        #\n",
    "        # Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Accumulation (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-accumulation [Accessed 9 November 2024].\n",
    "        # Code reused from: Hugging Face, n.d. Methods and tools for efficient training on a single GPU. Gradient Checkpointing (v.4.46.0) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.0/en/perf_train_gpu_one#gradient-checkpointing [Accessed 9 November 2024].\n",
    "        gradient_accumulation_steps = 4,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        save_strategy = \"epoch\",\n",
    "        #\n",
    "        # Code, that is, the parameter, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code is based on: huggingface, 2024. training_args.py (v.4.46.2) [computer program].\n",
    "        # Available from: https://github.com/huggingface/transformers/blob/v4.46.2/src/transformers/training_args.py [Accessed 9 November 2024].\n",
    "        # Line 1397\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        eval_strategy = \"epoch\",\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        logging_dir = \"ToT-tuning-prints-1\",\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        logging_steps = 100,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, the value for the parameter, adapted from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        num_train_epochs = 7.0,\n",
    "        #\n",
    "        # Code, that is, the parameter, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "        # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        # Code, that is, the value, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "        report_to = \"tensorboard\",\n",
    "        #\n",
    "        # Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "        # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 12 November 2024].\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 12 November 2024].\n",
    "        # Code is based on: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 12 November 2024].\n",
    "        # Code, that is, the parameter, reused from: Hugging Face, n.d. Optimization. Schedules. Learning Rate Schedules (PyTorch). class transformers.SchedulerType (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.2/en/main_classes/optimizer_schedules#transformers.SchedulerType [Accessed 12 November 2024].\n",
    "        # Code, that is, the value, reused from: Hugging Face, n.d. Optimization. Schedules. Learning Rate Schedules (PyTorch). class transformers.SchedulerType (v.4.46.2) [Online].\n",
    "        # Available from: https://huggingface.co/docs/transformers/v4.46.2/en/main_classes/optimizer_schedules#transformers.SchedulerType [Accessed 12 November 2024].\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        #\n",
    "    )\n",
    "    #\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "tuner.train()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: TensorFlow, 2023. Get started with TensorBoard [Online].\n",
    "# Available from: https://www.tensorflow.org/tensorboard/get_started [Accessed 9 November 2024].\n",
    "%load_ext tensorboard\n",
    "#\n",
    "\n",
    "# Code adapted from: TensorFlow, 2023. Get started with TensorBoard [Online].\n",
    "# Available from: https://www.tensorflow.org/tensorboard/get_started [Accessed 9 November 2024].\n",
    "# Code is based on: TensorFlow, 2023. Get started with TensorBoard [Online].\n",
    "# Available from: https://www.tensorflow.org/tensorboard/get_started [Accessed 9 November 2024].\n",
    "# Code is based on: Hugging Face, n.d. Trainer. TrainingArguments. class transformers.TrainingArguments (v.4.46.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments [Accessed 9 November 2024].\n",
    "%tensorboard --logdir ToT-tuning-prints-1\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reboot this notebook after tuning is completed (NVIDIA, 2024).\n",
    "\n",
    "NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is in line with: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register at (Hugging Face, n.d.a), perform the access request at (Mistral AI, 2024), follow (Hugging Face, n.d.b) to insert your private secret in the below code instead of \"\" as indicated in (Hugging Face, n.d.b).\n",
    "\n",
    "Hugging Face, n.d.a. The AI community building the future [Online]. Available from: https://huggingface.co [Accessed 19 October 2024].\n",
    "\n",
    "Hugging Face, n.d.b. User access tokens [Online]. Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "private_secret = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "import os\n",
    "#\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code, that is, the value, reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm = AutoModelForCausalLM.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm = AutoTokenizer.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code is based on: Hugging Face, n.d. Accessing Private/Gated Models (v.3.0.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers.js/en/guides/private [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter and value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the parameter and value, are in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.pad_token = tuning_tokenizer_for_llm.unk_token\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.padding_side = \"right\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "first_extracted_index = \"input_ids\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "second_extracted_index = 1\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "extracted_value = tuning_tokenizer_for_llm(tuning_tokenizer_for_llm.unk_token)[first_extracted_index][second_extracted_index]\n",
    "tuning_llm.config.pad_token_id = extracted_value\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from peft import PeftModel\n",
    "#\n",
    "\n",
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "combined_llm = PeftModel.from_pretrained(tuning_llm, \"ToT-tuning-1/checkpoint-643\")\n",
    "combined_llm.save_pretrained(\"ToT-tuning-1/tuned-1\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from peft import PeftModel\n",
    "#\n",
    "\n",
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "combined_llm = PeftModel.from_pretrained(tuning_llm, \"ToT-tuning-1/checkpoint-4501\")\n",
    "combined_llm.save_pretrained(\"ToT-tuning-1/tuned-2\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
