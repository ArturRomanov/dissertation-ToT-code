{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT test Mistral tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests tuned Mistral locally with base (Mistral AI, 2024) and a checkpoint from the ToT-tuning-1 notebook based on test dataset produced from the notebook ToT-data-ETL based on the MMLU dataset (Hendrycks et al, 2021a; Hendrycks et al, 2021b; Hendrycks et al, 2023)\n",
    "\n",
    "You should run the ToT-data-ETL before running this notebook\n",
    "\n",
    "You should also have a checkpoint from the ToT-tuning-1 folder from the ToT-tuning-1.ipynb notebook\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    " \n",
    "Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code in the Configure and Load sections in this notebook is in line with: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register at (Hugging Face, n.d.a), perform the access request at (Mistral AI, 2024), follow (Hugging Face, n.d.b) to insert your private secret in the below code instead of \"\" as indicated in (Hugging Face, n.d.b).\n",
    "\n",
    "Hugging Face, n.d.a. The AI community building the future [Online]. Available from: https://huggingface.co [Accessed 19 October 2024].\n",
    "\n",
    "Hugging Face, n.d.b. User access tokens [Online]. Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "private_secret = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artur/Downloads/Dissertation/dissertation-ToT-code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Code reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "\n",
    "# Code is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "import os\n",
    "#\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code, that is, the value, reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_llm = AutoModelForCausalLM.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm = AutoTokenizer.from_pretrained(tuning_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code is based on: Hugging Face, n.d. Accessing Private/Gated Models (v.3.0.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers.js/en/guides/private [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the parameter, is in line with and reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "# Code, that is, the value, is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter and value, adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the parameter and value, are in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.pad_token = tuning_tokenizer_for_llm.unk_token\n",
    "#\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "# Code is in line with and adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "tuning_tokenizer_for_llm.padding_side = \"right\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "first_extracted_index = \"input_ids\"\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "# Code, that is, the value, reused from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "second_extracted_index = 1\n",
    "#\n",
    "\n",
    "# Code adapted from: AnsonKw, 2024. mistral training code (v.1) [computer program].\n",
    "# Availabel from: https://www.kaggle.com/code/ansonkw/mistral-training-code [Accessed 26 October 2024].\n",
    "extracted_value = tuning_tokenizer_for_llm(tuning_tokenizer_for_llm.unk_token)[first_extracted_index][second_extracted_index]\n",
    "tuning_llm.config.pad_token_id = extracted_value\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "from peft import PeftModel\n",
    "#\n",
    "\n",
    "# Code adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "# Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 16 November 2024].\n",
    "combined_llm = PeftModel.from_pretrained(tuning_llm, \"ToT-tuning-1/checkpoint-643\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset_path_csv = \"full_test_dataset.csv\"\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset = pd.read_csv(test_dataset_path_csv)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_generated_answers = test_dataset[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] The question is: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A. 0\n",
      "B. 4\n",
      "C. 2\n",
      "D. 6\n",
      "What is the answer and the answer letter? [/INST]  #####\n",
      "Step 1\n",
      "\n",
      "Expert 1\n",
      "concise thought 1: The degree of a field extension is the dimension of the extension field as a vector space over the base field.\n",
      "concise critique of thought 1: This is a basic definition, but it doesn't help us find the degree in this case.\n",
      "probability of thought 1 being correct in percentage: 100%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2: We can find the degree by finding the number of linearly independent elements in the extension field over the base field.\n",
      "concise critique of thought 2: This is a good approach, but we need to find these elements first.\n",
      "probability of thought 2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3: The degree of a field extension is the same as the degree of the minimal polynomial of any element in the extension field over the base field.\n",
      "concise critique of thought 3: This is a good approach, but we need to find the minimal polynomial first.\n",
      "probability of thought 3 being correct in percentage: 80%\n",
      "\n",
      "Step 2\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.1: The elements sqrt(2), sqrt(3), and sqrt(18) are linearly independent over Q.\n",
      "concise critique of thought 1.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 1.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.2: The degree of the field extension Q(sqrt(2), sqrt(3)) over Q is 4, since sqrt(2) and sqrt(3) are linearly independent over Q.\n",
      "concise critique of thought 1.2: This is a basic fact, but we need to prove it.\n",
      "probability of thought 1.2 being correct in percentage: 90%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.1: The degree of the field extension Q(sqrt(18)) over Q is 2, since sqrt(18) is a root of the polynomial x^2 - 18.\n",
      "concise critique of thought 2.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 2.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.2: The degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q(sqrt(2), sqrt(3)) is 2, since sqrt(18) is a root of the polynomial x^2 - 18 and is not in Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 2.2: This is a good approach, but we need to prove it.\n",
      "probability of thought 2.2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.1: The minimal polynomial of sqrt(2) over Q is x^2 - 2.\n",
      "concise critique of thought 3.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 3.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.2: The minimal polynomial of sqrt(3) over Q(sqrt(2)) is x^2 - 3.\n",
      "concise critique of thought 3.2: This is a good approach, but we need to prove it.\n",
      "probability of thought 3.2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.3: The minimal polynomial of sqrt(18) over Q(sqrt(2), sqrt(3)) is x^2 - 18.\n",
      "concise critique of thought 3.3: This is a good approach, but we need to prove it.\n",
      "probability of thought 3.3 being correct in percentage: 80%\n",
      "\n",
      "Step 3\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.1.1: We can prove that sqrt(2) and sqrt(3) are linearly independent over Q by showing that their linear combination is not equal to 0.\n",
      "concise critique of thought 1.1.1: This is a good approach, but we need to find a specific linear combination that is not equal to 0.\n",
      "probability of thought 1.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.2.1: We can prove that the degree of the field extension Q(sqrt(2), sqrt(3)) over Q is 4 by showing that there are no elements in Q(sqrt(2), sqrt(3)) that are linearly independent over Q and have degree less than 4.\n",
      "concise critique of thought 1.2.1: This is a good approach, but we need to find such elements and show that they are not linearly independent over Q.\n",
      "probability of thought 1.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.1.1: We can prove that the degree of the field extension Q(sqrt(18)) over Q is 2 by showing that sqrt(18) is a root of the polynomial x^2 - 18 and has degree 2 over Q.\n",
      "concise critique of thought 2.1.1: This is a good approach, but we need to prove that sqrt(18) has degree 2 over Q.\n",
      "probability of thought 2.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.2.1: We can prove that the degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q(sqrt(2), sqrt(3)) is 2 by showing that sqrt(18) is not in Q(sqrt(2), sqrt(3)) and has degree 2 over Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 2.2.1: This is a good approach, but we need to prove that sqrt(18) has degree 2 over Q(sqrt(2), sqrt(3)).\n",
      "probability of thought 2.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.1.1: We can prove that the minimal polynomial of sqrt(2) over Q is x^2 - 2 by showing that it has degree 2 and is irreducible over Q.\n",
      "concise critique of thought 3.1.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q.\n",
      "probability of thought 3.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.2.1: We can prove that the minimal polynomial of sqrt(3) over Q(sqrt(2)) is x^2 - 3 by showing that it has degree 2 and is irreducible over Q(sqrt(2)).\n",
      "concise critique of thought 3.2.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q(sqrt(2)).\n",
      "probability of thought 3.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.3.1: We can prove that the minimal polynomial of sqrt(18) over Q(sqrt(2), sqrt(3)) is x^2 - 18 by showing that it has degree 2 and is irreducible over Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 3.3.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q(sqrt(2), sqrt(3)).\n",
      "probability of thought 3.3.1 being correct in percentage: 80%\n",
      "\n",
      "Answer: B\n",
      "What is the answer and the answer letter? [/INST] \n",
      " #####\n",
      "Step 1\n",
      "\n",
      "Expert 1\n",
      "concise thought 1: The degree of a field extension is the dimension of the extension field as a vector space over the base field.\n",
      "concise critique of thought 1: This is a basic definition, but it doesn't help us find the degree in this case.\n",
      "probability of thought 1 being correct in percentage: 100%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2: We can find the degree by finding the number of linearly independent elements in the extension field over the base field.\n",
      "concise critique of thought 2: This is a good approach, but we need to find these elements first.\n",
      "probability of thought 2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3: The degree of a field extension is the same as the degree of the minimal polynomial of any element in the extension field over the base field.\n",
      "concise critique of thought 3: This is a good approach, but we need to find the minimal polynomial first.\n",
      "probability of thought 3 being correct in percentage: 80%\n",
      "\n",
      "Step 2\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.1: The elements sqrt(2), sqrt(3), and sqrt(18) are linearly independent over Q.\n",
      "concise critique of thought 1.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 1.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.2: The degree of the field extension Q(sqrt(2), sqrt(3)) over Q is 4, since sqrt(2) and sqrt(3) are linearly independent over Q.\n",
      "concise critique of thought 1.2: This is a basic fact, but we need to prove it.\n",
      "probability of thought 1.2 being correct in percentage: 90%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.1: The degree of the field extension Q(sqrt(18)) over Q is 2, since sqrt(18) is a root of the polynomial x^2 - 18.\n",
      "concise critique of thought 2.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 2.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.2: The degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q(sqrt(2), sqrt(3)) is 2, since sqrt(18) is a root of the polynomial x^2 - 18 and is not in Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 2.2: This is a good approach, but we need to prove it.\n",
      "probability of thought 2.2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.1: The minimal polynomial of sqrt(2) over Q is x^2 - 2.\n",
      "concise critique of thought 3.1: This is a basic fact, but we need to prove it.\n",
      "probability of thought 3.1 being correct in percentage: 90%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.2: The minimal polynomial of sqrt(3) over Q(sqrt(2)) is x^2 - 3.\n",
      "concise critique of thought 3.2: This is a good approach, but we need to prove it.\n",
      "probability of thought 3.2 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.3: The minimal polynomial of sqrt(18) over Q(sqrt(2), sqrt(3)) is x^2 - 18.\n",
      "concise critique of thought 3.3: This is a good approach, but we need to prove it.\n",
      "probability of thought 3.3 being correct in percentage: 80%\n",
      "\n",
      "Step 3\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.1.1: We can prove that sqrt(2) and sqrt(3) are linearly independent over Q by showing that their linear combination is not equal to 0.\n",
      "concise critique of thought 1.1.1: This is a good approach, but we need to find a specific linear combination that is not equal to 0.\n",
      "probability of thought 1.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 1\n",
      "concise thought 1.2.1: We can prove that the degree of the field extension Q(sqrt(2), sqrt(3)) over Q is 4 by showing that there are no elements in Q(sqrt(2), sqrt(3)) that are linearly independent over Q and have degree less than 4.\n",
      "concise critique of thought 1.2.1: This is a good approach, but we need to find such elements and show that they are not linearly independent over Q.\n",
      "probability of thought 1.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.1.1: We can prove that the degree of the field extension Q(sqrt(18)) over Q is 2 by showing that sqrt(18) is a root of the polynomial x^2 - 18 and has degree 2 over Q.\n",
      "concise critique of thought 2.1.1: This is a good approach, but we need to prove that sqrt(18) has degree 2 over Q.\n",
      "probability of thought 2.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 2\n",
      "concise thought 2.2.1: We can prove that the degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q(sqrt(2), sqrt(3)) is 2 by showing that sqrt(18) is not in Q(sqrt(2), sqrt(3)) and has degree 2 over Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 2.2.1: This is a good approach, but we need to prove that sqrt(18) has degree 2 over Q(sqrt(2), sqrt(3)).\n",
      "probability of thought 2.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.1.1: We can prove that the minimal polynomial of sqrt(2) over Q is x^2 - 2 by showing that it has degree 2 and is irreducible over Q.\n",
      "concise critique of thought 3.1.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q.\n",
      "probability of thought 3.1.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.2.1: We can prove that the minimal polynomial of sqrt(3) over Q(sqrt(2)) is x^2 - 3 by showing that it has degree 2 and is irreducible over Q(sqrt(2)).\n",
      "concise critique of thought 3.2.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q(sqrt(2)).\n",
      "probability of thought 3.2.1 being correct in percentage: 80%\n",
      "\n",
      "Expert 3\n",
      "concise thought 3.3.1: We can prove that the minimal polynomial of sqrt(18) over Q(sqrt(2), sqrt(3)) is x^2 - 18 by showing that it has degree 2 and is irreducible over Q(sqrt(2), sqrt(3)).\n",
      "concise critique of thought 3.3.1: This is a good approach, but we need to prove that it has degree 2 and is irreducible over Q(sqrt(2), sqrt(3)).\n",
      "probability of thought 3.3.1 being correct in percentage: 80%\n",
      "\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "# Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "from transformers import pipeline\n",
    "#\n",
    "\n",
    "# Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "for dataset_record in dataset_for_generated_answers.itertuples():\n",
    "#\n",
    "    # Code, that is, the value, reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    element_before_ask = \"The question is: \"\n",
    "    #\n",
    "    # Code, that is, the value, is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 25)\n",
    "    # Code, that is, the value, is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    element_after_ask = \"What is the answer and the answer letter?\"\n",
    "    #\n",
    "    # Code, that is, the parameter, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, adapted from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is based on: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code, that is, the value, is based on transformations in ToT-data-ETL notebook to data from and according to: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    # Code, that is, the first, second, fifth, seventh elements of value, reused from: NVIDIA, 2024. Fine-tuning Mistral 7B using QLoRA (mistral-finetune.ipynb) [computer program].\n",
    "    # Available from: https://github.com/NVIDIA/workbench-example-mistral-finetune/blob/main/code/mistral-finetune.ipynb [Accessed 2 November 2024].\n",
    "    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, first, second and seventh strings, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    enhanced_prompt_to_generate_answer = \"<s>\" + \"[INST] \" + element_before_ask + dataset_record.enhanced_question + \"\\n\" + element_after_ask + \" [/INST] \"\n",
    "    #\n",
    "    # Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "    generated_answer_type = \"text-generation\"\n",
    "    #\n",
    "    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    answer_generator = pipeline(model = combined_llm, \n",
    "    tokenizer = tuning_tokenizer_for_llm, \n",
    "    task = generated_answer_type,\n",
    "    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "    device = \"mps\",\n",
    "    #\n",
    "    # Code is based on: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code is based on: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, adapted from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "    max_new_tokens = 5000\n",
    "    #\n",
    "    )\n",
    "    #\n",
    "    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    answer_first_index = 0\n",
    "    #\n",
    "    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    answer_second_index = \"generated_text\"\n",
    "    #\n",
    "    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "    generated_answer_with_enhanced_prompt = answer_generator(enhanced_prompt_to_generate_answer)[answer_first_index][answer_second_index]\n",
    "    print(generated_answer_with_enhanced_prompt)\n",
    "    #\n",
    "    enhanced_prompt_to_generate_answer_part = enhanced_prompt_to_generate_answer[(len(enhanced_prompt_to_generate_answer)-50):len(enhanced_prompt_to_generate_answer)]\n",
    "    print(enhanced_prompt_to_generate_answer_part)\n",
    "    generated_answer = generated_answer_with_enhanced_prompt.split(enhanced_prompt_to_generate_answer_part)[1]\n",
    "    print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
