{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT Evaluate Mistral\n",
    "\n",
    "This notebook evaluates Mistral locally with (Mistral AI, 2024) for the generation of answers and (Ollama, 2024a; Ollama, 2024b) for other functions based on test dataset produced from the ToT-data-answer-generator-and-checker notebook and based on the MMLU dataset (Hendrycks et al, 2021a; Hendrycks et al, 2021b; Hendrycks et al, 2023)\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "\n",
    "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    " \n",
    "Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "\n",
    "Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "\n",
    "Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register at (Hugging Face, n.d.a), perform the access request at (Mistral AI, 2024), follow (Hugging Face, n.d.b) to insert your private secret in the below code instead of \"\" as indicated in (Hugging Face, n.d.b).\n",
    "\n",
    "Hugging Face, n.d.a. The AI community building the future [Online]. Available from: https://huggingface.co [Accessed 19 October 2024].\n",
    "\n",
    "Hugging Face, n.d.b. User access tokens [Online]. Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "\n",
    "Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "private_secret = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Code, that is, the parameter, adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "# Code, that is, the value, reused from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "evaluation_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "evaluation_llm = AutoModelForCausalLM.from_pretrained(evaluation_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoModelForCausalLM. from_pretrained (v.4.45.2) [Online].\n",
    "# Availabel from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained [Accessed 19 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n",
    "\n",
    "# Code adapted from: Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. \n",
    "# Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "evaluation_tokenizer_for_llm = AutoTokenizer.from_pretrained(evaluation_llm_name,\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "# Code, that is, the value, is based on: Hugging Face, n.d. Auto Classes. AutoTokenizer. from_pretrained (v.4.45.2) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained [Accessed 20 October 2024].\n",
    "cache_dir = current_path,\n",
    "#\n",
    "# Code is based on: Hugging Face, n.d. Accessing Private/Gated Models (v.3.0.0) [Online].\n",
    "# Available from: https://huggingface.co/docs/transformers.js/en/guides/private [Accessed 20 October 2024].\n",
    "# Code, that is, the parameter, reused from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "# Code, that is, the value, adapted from: Hugging Face, n.d. User access tokens [Online]. \n",
    "# Available from: https://huggingface.co/docs/hub/en/security-tokens [Accessed 19 October 2024].\n",
    "token = private_secret\n",
    "#\n",
    ")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset_path_csv = \"test-dataset-for-tuning/test_dataset_0_0_671.csv\"\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset = pd.read_csv(test_dataset_path_csv)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the evaluation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"string_answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"llm_answer_check\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"llm_answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 17 August 2024].\n",
    "test_dataset.loc[:, \"answer_evaluation\"] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_simple(enhanced_question):\n",
    "    # Prompt (lines 1 - 2 in the simple_prompt_to_generate_answer) is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Prompt (lines 1 - 2 in the simple_prompt_to_generate_answer) is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 23 - 25)\n",
    "    #\n",
    "    # Please note that enhanced_question variable in the simple_prompt_to_generate_answer would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    simple_prompt_to_generate_answer = f'''\n",
    "    {enhanced_question}\n",
    "    What is the answer and the answer letter? It is very important that you provide the correct answer letter in the format of Answer: A, Answer: B, Answer: C or Answer: D\n",
    "    '''\n",
    "    #\n",
    "    print(simple_prompt_to_generate_answer)\n",
    "    return simple_prompt_to_generate_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToT prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tot(enhanced_question):\n",
    "    # Prompt (lines 1 - 11 in the tot_prompt_to_generate_answer) reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 11 - 22)\n",
    "    #\n",
    "    # Prompt (lines 16 - 99 in the tot_prompt_to_generate_answer, that is, where delimited by #####) is based on: Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y. and Narasimhan, K., 2023. \n",
    "    # Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 37th Conference on Neural Information Processing Systems (NeurIPS 2023), 10-16 December 2023, New Orleans. \n",
    "    # Ithaca: Cornell University Library, arXiv.org, pp.1-14. Available from: https://arxiv.org/pdf/2305.10601.pdf [Accessed 17 August 2024].\n",
    "    #\n",
    "    # Prompt (lines 16 - 99 in the tot_prompt_to_generate_answer, that is, where delimited by #####) is based on the approach of how tree is structured: Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y. and Narasimhan, K., 2023. \n",
    "    # Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 37th Conference on Neural Information Processing Systems (NeurIPS 2023), 10-16 December 2023, New Orleans. \n",
    "    # Ithaca: Cornell University Library, arXiv.org, pp.1-14. Available from: https://arxiv.org/pdf/2305.10601.pdf [Accessed 17 August 2024]. \n",
    "    # (for example, Yao et al, 2023, page 2, figure d)\n",
    "    #\n",
    "    # Prompt (lines 13 - 105, line 109 in the tot_prompt_to_generate_answer) is adapted and based on the ToT pattern according to: Zhang, Z., Ye, Z., Shen, Y. and Gan, C., 2023. \n",
    "    # Autonomous Tree-Search Ability of Large Language Models. Ithaca: Cornell University Library, arXiv.org. arXiv [Online]. \n",
    "    # Available from: https://arxiv.org/pdf/2310.10686.pdf [Accessed 25 August 2024]. \n",
    "    # (Zhang et al, 2023, page 13, C.1)\n",
    "    #\n",
    "    # Prompt (lines 16 - 106 in the tot_prompt_to_generate_answer) is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 11 - 26)\n",
    "    #\n",
    "    # Prompt (line 102 in the tot_prompt_to_generate_answer, that is, where indicated about the letters and line 11, line 97, line 99, line 106, line 108, line 109 in the tot_prompt_to_generate_answer) is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Prompt (line 108 in the tot_prompt_to_generate_answer) reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    #\n",
    "    # Prompt (line 109 in the tot_prompt_to_generate_answer) is adapted and based on: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, lines 11 - 26)\n",
    "    #\n",
    "    # Prompt (line 109, that is, before asking about the answer) is based on: Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2023. \n",
    "    # Large Language Models are Zero-Shot Reasoners. 36th Conference on Neural Information Processing Systems (NeurIPS 2022), 28 November 2022 – 9 December 2022, New Orleans. \n",
    "    # Ithaca: Cornell University Library, arXiv.org, pp.1-42. Available from: https://arxiv.org/pdf/2205.11916 [Accessed 15 September 2024]. \n",
    "    # (Kojima et al, 2023, page 2, figure d)\n",
    "    #\n",
    "    # Please note that enhanced_question variable in the tot_prompt_to_generate_answer would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    tot_prompt_to_generate_answer = f'''\n",
    "    Imagine three different experts are answering this question in the Tree of Thoughts style.\n",
    "    They will brainstorm and debate the answer step by step reasoning carefully and taking all facts into consideration\n",
    "    All experts will write down 1 step of their thinking, then share it with the group.\n",
    "    They will each critique their response, and then all the responses of others\n",
    "    They will check their answer based on the appropriate rules.\n",
    "    Then all experts will go on to the next step and write down this step of their thinking.\n",
    "    They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts.\n",
    "    If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred. \n",
    "    If any expert realises they're wrong at any point then they acknowledge this and start another train of thought.\n",
    "    Each expert will assign a likelihood of their current assertion being correct.\n",
    "    Continue until the experts agree on the single most likely answer\n",
    "\n",
    "    Use the following template delimited by #####.\n",
    "\n",
    "    #####\n",
    "    Step 1\n",
    "\n",
    "    Expert 1\n",
    "    concise thought 1:\n",
    "    concise critique of thought 1:\n",
    "    probability of thought 1 being correct in percentage:\n",
    "\n",
    "    Expert 2\n",
    "    concise thought 2:\n",
    "    concise critique of thought 2:\n",
    "    probability of thought 2 being correct in percentage:\n",
    "\n",
    "    Expert 3\n",
    "    concise thought 3:\n",
    "    concise critique of thought 3:\n",
    "    probability of thought 3 being correct in percentage:\n",
    "\n",
    "    Step 2\n",
    "\n",
    "    Expert 1\n",
    "    concise thought 1.1:\n",
    "    concise critique of thought 1.1:\n",
    "    probability of thought 1.1 being correct in percentage:\n",
    "\n",
    "    Expert 1\n",
    "    concise thought 1.2:\n",
    "    concise critique of thought 1.2:\n",
    "    probability of thought 1.2 being correct in percentage:\n",
    "\n",
    "    Expert 2\n",
    "    concise thought 2.1:\n",
    "    concise critique of thought 2.1:\n",
    "    probability of thought 2.1 being correct in percentage:\n",
    "\n",
    "    Expert 2\n",
    "    concise thought 2.2:\n",
    "    concise critique of thought 2.2:\n",
    "    probability of thought 2.2 being correct in percentage:\n",
    "\n",
    "    Expert 3\n",
    "    concise thought 3.1:\n",
    "    concise critique of thought 3.1:\n",
    "    probability of thought 3.1 being correct in percentage:\n",
    "\n",
    "    Expert 3\n",
    "    concise thought 3.2:\n",
    "    concise critique of thought 3.2:\n",
    "    probability of thought 3.2 being correct in percentage:\n",
    "\n",
    "    Step 3\n",
    "\n",
    "    Expert 1\n",
    "    concise thought 1.1.1:\n",
    "    concise critique of thought 1.1.1:\n",
    "    probability of thought 1.1.1 being correct in percentage:\n",
    "\n",
    "    Expert 1\n",
    "    concise thought 1.2.1:\n",
    "    concise critique of thought 1.2.1:\n",
    "    probability of thought 1.2.1 being correct in percentage:\n",
    "\n",
    "    Expert 2\n",
    "    concise thought 2.1.1:\n",
    "    concise critique of thought 2.1.1:\n",
    "    probability of thought 2.1.1 being correct in percentage:\n",
    "\n",
    "    Expert 2\n",
    "    concise thought 2.2.1:\n",
    "    concise critique of thought 2.2.1:\n",
    "    probability of thought 2.2.1 being correct in percentage:\n",
    "\n",
    "    Expert 3\n",
    "    concise thought 3.1.1:\n",
    "    concise critique of thought 3.1.1:\n",
    "    probability of thought 3.1.1 being correct in percentage:\n",
    "\n",
    "    Expert 3\n",
    "    concise thought 3.2.1:\n",
    "    concise critique of thought 3.2.1:\n",
    "    probability of thought 3.2.1 being correct in percentage:\n",
    "\n",
    "    Conclusion: write here the answer which all of the experts agree is the correct answer based on the final thoughts of the experts\n",
    "\n",
    "    Answer: write here the answer letter which all of the experts agree is the correct answer based on the final thoughts of the experts\n",
    "    #####\n",
    "\n",
    "    Very important. The thoughts of experts must be diverse and different, that is, the experts must not repeat what they previously said and the thoughts of the experts should include letters (A, B, C or D) to which the thoughts refer or relate. \n",
    "    Very important. In Step 1, each expert must provide 1 thought (3 thoughts in total, that is, concise thought 1, concise thought 2, concise thought 3). \n",
    "    Very important. In Step 2, each expert must provide 2 thoughts (6 thoughts in total, that is, concise thought 1.1, concise thought 1.2, concise thought 2.1, concise thought 2.2, concise thought 3.1, concise thought 3.2). \n",
    "    Very important. In Step 3, each expert must provide at least 2 thoughts (at least 6 thoughts in total), etc.\n",
    "    Very important. The answer letter which all of the experts agree is the correct answer based on the final thoughts of the experts should be in the format of Answer: answer letter which all of the experts agree is the correct answer based on the final thoughts of the experts, for example, Answer: A, Answer: B, Answer: C or Answer: D.\n",
    "\n",
    "    The question is: {enhanced_question}\n",
    "    Think in the Tree of Thoughts style. What is the answer and the answer letter? It is very important that you provide the correct answer letter in the format of Answer: A, Answer: B, Answer: C or Answer: D and it is very important that you provide it only after all the thoughts in step 3\n",
    "    '''\n",
    "    #\n",
    "    print(tot_prompt_to_generate_answer)\n",
    "    return tot_prompt_to_generate_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_simple_evaluation(enhanced_question, generated_simple_answer, letter_answer, letter_answer_text):\n",
    "    # Prompt (lines 1, 2, 5, 8, 9, 10 in the simple_evaluation_prompt) is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Prompt (line 1 in the simple_evaluation_prompt) reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    #\n",
    "    # Please note that enhanced_question variable in the simple_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_simple_answer variable in the simple_evaluation_prompt would include an answer based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_simple_answer variable in the simple_evaluation_prompt would include an answer from Mistral 7B Instruct version v0.3 at the period of running this notebook and which would be used locally using:\n",
    "    # Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024]. \n",
    "    # simple_prompt_to_generate_answer in prompt_simple function in Simple prompt section of this notebook\n",
    "    # enhanced_question values for the prompt from test_dataset in the notebook which is based on the MMLU dataset:\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer variable in the simple_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer_text variable in the simple_evaluation_prompt would include data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    simple_evaluation_prompt = f'''\n",
    "    The question is: {enhanced_question}\n",
    "    The following text delimited by ####### is the answer to the question.\n",
    "\n",
    "    #######\n",
    "    {generated_simple_answer}\n",
    "    #######\n",
    "\n",
    "    Evaluate the text delimited by ####### which is the answer to the question and conclude if the answer of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}.\n",
    "    If the answer of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}, output correct.\n",
    "    Otherwise, if the answer of the text delimited by ####### is not {letter_answer} and/or {letter_answer_text}, output incorrect.\n",
    "    You must output only one word: correct or incorrect.\n",
    "    '''\n",
    "    print(simple_evaluation_prompt)\n",
    "    return simple_evaluation_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToT evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tot_evaluation(enhanced_question, generated_tot_answer, letter_answer, letter_answer_text):\n",
    "    # Prompt (lines 1, 2, 5, 8, 9, 10 in the tot_evaluation_prompt) is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Prompt (line 1 in the tot_evaluation_prompt) reused and slightly adapted from: mrspiggot, 2023. langchain_tree.py [computer program].\n",
    "    # Available from: https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_tree.py [Accessed 5 September 2024]. \n",
    "    # (mrspiggot, 2023, line 23)\n",
    "    #\n",
    "    # Please note that enhanced_question variable in the tot_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_tot_answer variable in the tot_evaluation_prompt would include an answer based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that generated_tot_answer variable in the tot_evaluation_prompt would include an answer from Mistral 7B Instruct version v0.3 at the period of running this notebook and which would be used locally using:\n",
    "    # Mistral AI, 2024. Model Card for Mistral-7B-Instruct-v0.3 [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 [Accessed 19 October 2024].\n",
    "    # tot_prompt_to_generate_answer in prompt_tot function in ToT prompt section of this notebook\n",
    "    # enhanced_question values for the prompt from test_dataset in the notebook which is based on the MMLU dataset:\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "    # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "    # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer variable in the tot_evaluation_prompt would include transformed data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    # Please note that letter_answer_text variable in the tot_evaluation_prompt would include data from: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "    # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "    #\n",
    "    tot_evaluation_prompt = f'''\n",
    "    The question is: {enhanced_question}\n",
    "    The following text delimited by ####### is the answer to the question.\n",
    "\n",
    "    #######\n",
    "    {generated_tot_answer}\n",
    "    #######\n",
    "\n",
    "    Evaluate the text delimited by ####### which is the answer to the question and conclude if the answer provided at the bottom of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}.\n",
    "    If the answer provided at the bottom of the text delimited by ####### is {letter_answer} and/or {letter_answer_text}, output correct.\n",
    "    Otherwise, if the answer provided at the bottom of the text delimited by ####### is not {letter_answer} and/or {letter_answer_text}, output incorrect.\n",
    "    You must output only one word: correct or incorrect.\n",
    "    '''\n",
    "    print(tot_evaluation_prompt)\n",
    "    return tot_evaluation_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Evaluate Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders to save generated and evaluated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "def create_data_for_tuning_folder_address(current_path, data_for_tuning_folder_name):\n",
    "    return current_path + \"/\" + data_for_tuning_folder_name\n",
    "\n",
    "def create_folder_for_data_for_tuning(data_for_tuning_folder_name, data_for_tuning_folder_address):\n",
    "    # Code, that is, the check for folder, adapted from: Python Software Foundation, 2024. os.path - Common pathname manipulations. os.path.exists (v.3.12.5) [Online]. \n",
    "    # Available from: https://docs.python.org/3/library/os.path.html#os.path.exists [Accessed 25 August 2024].\n",
    "    data_for_tuning_folder_is_present = os.path.exists(data_for_tuning_folder_address)\n",
    "    #\n",
    "    \n",
    "    if not data_for_tuning_folder_is_present:\n",
    "        os.mkdir(data_for_tuning_folder_address)\n",
    "        print(f'Created {data_for_tuning_folder_name} folder')\n",
    "    else:\n",
    "        print(f'{data_for_tuning_folder_name} folder is present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if answers were already generated and load the appropriate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_and_load_generated_answers_to_continue(initial_dataset, data_for_tuning_folder_name, data_for_tuning_folder_address):\n",
    "    '''\n",
    "    This function checks if there are any files with the generated answers in a folder and if so, loads intermediate files to continue generating answers or loads a completed file, \n",
    "    otherwise uses the intial dataset to start generating answers from the beginning\n",
    "    '''\n",
    "    # Code, that is, the finding of files for tuning, adapted from: Python Software Foundation, 2024. os - Miscellaneous operating system interfaces. os.listdir (v.3.12.5) [Online]. \n",
    "    # Available from: https://docs.python.org/3/library/os.html#os.listdir [Accessed 31 August 2024].\n",
    "    files_for_tuning = os.listdir(data_for_tuning_folder_address)\n",
    "    #\n",
    "\n",
    "    if len(files_for_tuning):\n",
    "        # Code, that is, the function and the ordering of files for tuning, adapted from: Python Software Foundation, 2024. Built-in Types. sort (v.3.12.5) [Online]. \n",
    "        # Available from: https://docs.python.org/3/library/stdtypes.html#list.sort [Accessed 31 August 2024].\n",
    "        def check_version_of_file(file):\n",
    "            return int(file.split('_')[2])\n",
    "        \n",
    "        files_for_tuning.sort(key=check_version_of_file, reverse=True)\n",
    "        #\n",
    "        print(f\"The following files were found in the {data_for_tuning_folder_name} folder. {files_for_tuning}\")\n",
    "        recent_file_for_tuning = files_for_tuning[0]\n",
    "        print(f\"The most recent file in the {data_for_tuning_folder_name} folder is {recent_file_for_tuning}\")\n",
    "        recent_file_for_tuning_attributes = recent_file_for_tuning.split('_')\n",
    "\n",
    "        recent_file_for_tuning_address = data_for_tuning_folder_address + '/' + recent_file_for_tuning\n",
    "        # Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "        dataset_for_generated_answers = pd.read_csv(recent_file_for_tuning_address)\n",
    "        #\n",
    "        dataset_for_generated_answers_version = int(recent_file_for_tuning_attributes[2])\n",
    "        generated_answers_count = int(recent_file_for_tuning_attributes[3])\n",
    "        total_dataset_rows = int(recent_file_for_tuning_attributes[4].split('.')[0])\n",
    "        \n",
    "        if generated_answers_count < total_dataset_rows:\n",
    "            print(f\"The {data_for_tuning_folder_name} folder has intermediate files with generated answers. Continue the generation of answers. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        elif generated_answers_count == total_dataset_rows:\n",
    "            print(f\"The {data_for_tuning_folder_name} folder has already generated answers. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        return dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows\n",
    "    else:\n",
    "        dataset_for_generated_answers = initial_dataset\n",
    "        dataset_for_generated_answers_version = 0\n",
    "        generated_answers_count = 0\n",
    "        total_dataset_rows = len(dataset_for_generated_answers)\n",
    "        print(f\"The {data_for_tuning_folder_name} folder does not have files with generated answers. Start the generation of answers from the beginning. The dataset version is {dataset_for_generated_answers_version}. The count of generated answers is {generated_answers_count}. The total count of rows is {total_dataset_rows}.\")\n",
    "        return dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and evaluate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the import, reused from: LangChain, 2023. OllamaLLM [Online]. \n",
    "# Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "from langchain_ollama import OllamaLLM\n",
    "#\n",
    "# Code, that is, the import, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "# Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024].\n",
    "import numpy as np\n",
    "#\n",
    "\n",
    "# Code reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "# Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "from transformers import pipeline\n",
    "#\n",
    "\n",
    "def generate_and_check_answers(data_for_tuning_folder_name, data_for_tuning_folder_address, dataset_for_generated_answers, dataset_for_generated_answers_version, generated_answers_count, total_dataset_rows, tot):\n",
    "    '''\n",
    "    This function generates answers in the simple or ToT style to be used for evaluation, checks the answers for quality, saves the answers periodically with the intermediate versions\n",
    "    '''\n",
    "    if generated_answers_count < total_dataset_rows:\n",
    "        generated_answers_count_updated = generated_answers_count\n",
    "        dataset_for_generated_answers_version_updated = dataset_for_generated_answers_version\n",
    "        save_dataset_count = 15\n",
    "        # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "        for dataset_record in dataset_for_generated_answers.itertuples():\n",
    "        #\n",
    "            if (dataset_record.generated_answer is None or\n",
    "            # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "            # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "            dataset_record.generated_answer is np.nan or\n",
    "            #\n",
    "            # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "            pd.isna(dataset_record.generated_answer) == True):\n",
    "            #\n",
    "                retries_for_quality = 0\n",
    "                max_retries_for_quality = 3\n",
    "                while retries_for_quality < max_retries_for_quality:\n",
    "                    if tot:\n",
    "                        prompt_to_generate_answer = prompt_tot(dataset_record.enhanced_question)\n",
    "                    else:\n",
    "                        prompt_to_generate_answer = prompt_simple(dataset_record.enhanced_question)\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, first, second and fourth strings, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    enhanced_prompt_to_generate_answer = \"<s>\" + \"[INST] \" + prompt_to_generate_answer + \" [/INST]\"\n",
    "                    #\n",
    "                    # Code, that is, the parameter, adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    generated_answer_type = \"text-generation\"\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_generator = pipeline(model = evaluation_llm, \n",
    "                    tokenizer = evaluation_tokenizer_for_llm, \n",
    "                    task = generated_answer_type,\n",
    "                    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    device = \"mps\",\n",
    "                    #\n",
    "                    # Code is based on: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code is based on: Hugging Face, n.d. Pipelines. The pipeline abstraction. transformers.pipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.pipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the parameter, reused from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, adapted from: Hugging Face, n.d. Pipelines. Natural Language Processing. TextGenerationPipeline. class transformers.TextGenerationPipeline (v.4.45.2) [Online].\n",
    "                    # Available from: https://huggingface.co/docs/transformers/v4.45.2/main_classes/pipelines#transformers.TextGenerationPipeline [Accessed 20 October 2024].\n",
    "                    max_new_tokens = 5000\n",
    "                    #\n",
    "                    )\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_first_index = 0\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    # Code, that is, the value, reused from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    answer_second_index = \"generated_text\"\n",
    "                    #\n",
    "                    # Code adapted from: Awan, A.A., 2023. Mistral 7B Instruct 4bit QLoRA Fine-tuning (v.2) [computer program].\n",
    "                    # Available from: https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning [Accessed 20 October 2024].\n",
    "                    generated_answer_with_enhanced_prompt = answer_generator(enhanced_prompt_to_generate_answer)[answer_first_index][answer_second_index]\n",
    "                    print(generated_answer_with_enhanced_prompt)\n",
    "                    #\n",
    "                    if tot:\n",
    "                        enhanced_prompt_to_generate_answer_part = enhanced_prompt_to_generate_answer[(len(enhanced_prompt_to_generate_answer)-300):len(enhanced_prompt_to_generate_answer)]\n",
    "                    else:\n",
    "                        enhanced_prompt_to_generate_answer_part = enhanced_prompt_to_generate_answer[(len(enhanced_prompt_to_generate_answer)-180):len(enhanced_prompt_to_generate_answer)]\n",
    "                    print(enhanced_prompt_to_generate_answer_part)\n",
    "                    generated_answer = generated_answer_with_enhanced_prompt.split(enhanced_prompt_to_generate_answer_part)[1]\n",
    "                    print(generated_answer)\n",
    "                    if tot:\n",
    "                        # This simple check for quality checks that the generated answer has at least different second level thoughts based on the\n",
    "                        # Zhang, Z., Ye, Z., Shen, Y. and Gan, C., 2023. Autonomous Tree-Search Ability of Large Language Models. Ithaca: Cornell University Library, arXiv.org. arXiv [Online]. Available from: https://arxiv.org/pdf/2310.10686.pdf [Accessed 25 August 2024].\n",
    "                        # Page 13, C.1\n",
    "                        if ((\"1.1\" in generated_answer and \n",
    "                        \"1.2\" in generated_answer and \n",
    "                        \"2.1\" in generated_answer and \n",
    "                        \"2.2\" in generated_answer and \n",
    "                        \"3.1\" in generated_answer and \n",
    "                        \"3.2\" in generated_answer) and \n",
    "                        #\n",
    "                        # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "                        # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "                        # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "                        # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "                        # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "                        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "                        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "                        # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "                        # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "                        # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "                        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "                        # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                        (f\"Answer: A\" in generated_answer or\n",
    "                         f\"Answer: B\" in generated_answer or \n",
    "                         f\"Answer: C\" in generated_answer or  \n",
    "                         f\"Answer: D\" in generated_answer or\n",
    "                         f\"answer: A\" in generated_answer or\n",
    "                         f\"answer: B\" in generated_answer or\n",
    "                         f\"answer: C\" in generated_answer or\n",
    "                         f\"answer: D\" in generated_answer or\n",
    "                         f\"Answer is A\" in generated_answer or\n",
    "                         f\"Answer is B\" in generated_answer or\n",
    "                         f\"Answer is C\" in generated_answer or\n",
    "                         f\"Answer is D\" in generated_answer or\n",
    "                         f\"answer is A\" in generated_answer or\n",
    "                         f\"answer is B\" in generated_answer or\n",
    "                         f\"answer is C\" in generated_answer or\n",
    "                         f\"answer is D\" in generated_answer or\n",
    "                         f\"Answer is: A\" in generated_answer or\n",
    "                         f\"Answer is: B\" in generated_answer or\n",
    "                         f\"Answer is: C\" in generated_answer or\n",
    "                         f\"Answer is: D\" in generated_answer or\n",
    "                         f\"answer is: A\" in generated_answer or\n",
    "                         f\"answer is: B\" in generated_answer or\n",
    "                         f\"answer is: C\" in generated_answer or\n",
    "                         f\"answer is: D\" in generated_answer)):\n",
    "                        #\n",
    "                            retries_for_quality = max_retries_for_quality\n",
    "                        retries_for_quality += 1\n",
    "                    else:\n",
    "                        # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "                        # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "                        # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "                        # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "                        # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "                        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "                        # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "                        # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "                        # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "                        # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "                        # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "                        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "                        # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                        # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                        if (f\"Answer: A\" in generated_answer or\n",
    "                            f\"Answer: B\" in generated_answer or \n",
    "                            f\"Answer: C\" in generated_answer or  \n",
    "                            f\"Answer: D\" in generated_answer or\n",
    "                            f\"answer: A\" in generated_answer or\n",
    "                            f\"answer: B\" in generated_answer or\n",
    "                            f\"answer: C\" in generated_answer or\n",
    "                            f\"answer: D\" in generated_answer or\n",
    "                            f\"Answer is A\" in generated_answer or\n",
    "                            f\"Answer is B\" in generated_answer or\n",
    "                            f\"Answer is C\" in generated_answer or\n",
    "                            f\"Answer is D\" in generated_answer or\n",
    "                            f\"answer is A\" in generated_answer or\n",
    "                            f\"answer is B\" in generated_answer or\n",
    "                            f\"answer is C\" in generated_answer or\n",
    "                            f\"answer is D\" in generated_answer or\n",
    "                            f\"Answer is: A\" in generated_answer or\n",
    "                            f\"Answer is: B\" in generated_answer or\n",
    "                            f\"Answer is: C\" in generated_answer or\n",
    "                            f\"Answer is: D\" in generated_answer or\n",
    "                            f\"answer is: A\" in generated_answer or\n",
    "                            f\"answer is: B\" in generated_answer or\n",
    "                            f\"answer is: C\" in generated_answer or\n",
    "                            f\"answer is: D\" in generated_answer):\n",
    "                        #\n",
    "                            retries_for_quality = max_retries_for_quality\n",
    "                        retries_for_quality += 1\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_generated_answers.loc[dataset_for_generated_answers['id'] == dataset_record.id, \"generated_answer\"] = generated_answer\n",
    "                #\n",
    "                generated_answers_count_updated += 1\n",
    "                print(f\"The generated answer for {dataset_record.id} was generated with quality check and {retries_for_quality} retries\")\n",
    "                if (generated_answers_count_updated % save_dataset_count == 0) or (generated_answers_count_updated == total_dataset_rows):\n",
    "                    dataset_for_generated_answers_version_updated += 1\n",
    "                    dataset_type = data_for_tuning_folder_name.split('-')[0]\n",
    "                    dataset_for_generated_answers_address = data_for_tuning_folder_address + '/' + f'{dataset_type}_dataset_{dataset_for_generated_answers_version_updated}_{generated_answers_count_updated}_{total_dataset_rows}.csv'\n",
    "                    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "                    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "                    dataset_for_generated_answers.to_csv(dataset_for_generated_answers_address, index=False)\n",
    "                    #\n",
    "                    print(f\"The dataset for generated answers was saved for the version {dataset_for_generated_answers_version_updated}. The count of generated answers is {generated_answers_count_updated}. The count of total rows is {total_dataset_rows}\")\n",
    "            else:\n",
    "                print(f\"The generated answer for id {dataset_record.id} was already generated\")\n",
    "    elif generated_answers_count == total_dataset_rows:\n",
    "        print(\"The generated answers were already generated for the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, that is, the import, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "# Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024].\n",
    "import numpy as np\n",
    "#\n",
    "\n",
    "# Code, that is, the import, reused from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "# Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "import re\n",
    "#\n",
    "\n",
    "def string_answer_evaluate(data_for_tuning_folder_address, dataset_for_string_answer_evaluation_name, dataset_for_string_answer_evaluation, extract_first_answer):\n",
    "    '''\n",
    "    This function evaluates generated answers for the correct answer based on the string pattern of answers where 1 is correct answer and 0 is incorrect answer\n",
    "    '''\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in dataset_for_string_answer_evaluation.itertuples():\n",
    "    #\n",
    "        if (dataset_record.string_answer_evaluation is None or\n",
    "        # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "        # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "        dataset_record.string_answer_evaluation is np.nan or\n",
    "        #\n",
    "        # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "        pd.isna(dataset_record.string_answer_evaluation) == True):\n",
    "        #\n",
    "            if extract_first_answer:\n",
    "                print(f\"Extract first answer is used for id {dataset_record.id}\")\n",
    "                # Code is adapted from and based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "                # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "                # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "                # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "                # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "                # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "                # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "                # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "                # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "                # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "                # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "                # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "                # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "                # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "                # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "                string_answer_to_extract = re.compile(\"(Answer: |answer: |Answer is |answer is |Answer is: |answer is: )[A-D]\")\n",
    "                #\n",
    "                # Code is adapted from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                string_answer_check = string_answer_to_extract.search(dataset_record.generated_answer)\n",
    "                #\n",
    "                # Code is adapted from and based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                if string_answer_check is not None:\n",
    "                #\n",
    "                    print(f\"String answer check for id {dataset_record.id} is {string_answer_check}\")\n",
    "                    # Code is adapted from: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                    # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                    string_answer_indices = string_answer_check.span()\n",
    "                    #\n",
    "                    # Code is based on: Kuchling, A.M., 2024. Regular Expression HOWTO (v.3.13.0) [Online]. s.l.: Python Software Foundation.\n",
    "                    # Available from: https://docs.python.org/3/howto/regex.html [Accessed 15 October 2024].\n",
    "                    generated_answer = dataset_record.generated_answer[0:string_answer_indices[1]]\n",
    "                    #\n",
    "                else:\n",
    "                    generated_answer = dataset_record.generated_answer\n",
    "            else:\n",
    "                generated_answer = dataset_record.generated_answer\n",
    "\n",
    "            print(f\"The generated answer to be checked for id {dataset_record.id} is {generated_answer}\")\n",
    "\n",
    "            # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "            # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "            # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "            # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "            # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "            # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "            # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "            # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "            # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            if (f\"Answer: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"Answer is {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer is {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"Answer is: {dataset_record.letter_answer}\" in generated_answer or\n",
    "                f\"answer is: {dataset_record.letter_answer}\" in generated_answer):\n",
    "            #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = 1\n",
    "                #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = 1\n",
    "                #\n",
    "            # Code is based on several outputs from Mixtral 8x7b Instruct version v0.1, fp16 (pers. comm.) on 03/10/2024 \n",
    "            # from tot_prompt_to_generate_answer prompt in the prompt_tot function in the Prompt section in ToT-data-answer-generator-and-checker notebook, with and without hint_1 and hint_2 \n",
    "            # values for the prompt which are indicated in the code in Generate and check answers subsection in the ToT-data-answer-generator-and-checker notebook \n",
    "            # and enhanced_question values from train_dataset in the ToT-data-answer-generator-and-checker notebook\n",
    "            # for the prompt (that is, the outputs that could be produced with the code in the ToT-data-answer-generator-and-checker notebook) which is based on the MMLU dataset: \n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021b. Measuring Massive Multitask Language Understanding. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-27. Available from: https://arxiv.org/pdf/2009.03300.pdf [Accessed 5 August 2024].\n",
    "            # Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D. and Steinhardt, J., 2023. Aligning AI With Shared Human Values. \n",
    "            # ICLR 2021, 4 May 2021, Vienna. Ithaca: Cornell University Library, arXiv.org, pp.1-29. Available from: https://arxiv.org/pdf/2008.02275.pdf [Accessed 5 August 2024].\n",
    "            # Please note that Mixtral 8x7b Instruct version v0.1, fp16 has been used locally using: \n",
    "            # Ollama, 2024a. Ollama [computer program]. Available from: https://ollama.com [Accessed 1 September 2024].\n",
    "            # Ollama, 2024b. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "            # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            elif (f\"Answer: A\" in generated_answer or\n",
    "                f\"Answer: B\" in generated_answer or \n",
    "                f\"Answer: C\" in generated_answer or  \n",
    "                f\"Answer: D\" in generated_answer or\n",
    "                f\"answer: A\" in generated_answer or\n",
    "                f\"answer: B\" in generated_answer or\n",
    "                f\"answer: C\" in generated_answer or\n",
    "                f\"answer: D\" in generated_answer or\n",
    "                f\"Answer is A\" in generated_answer or\n",
    "                f\"Answer is B\" in generated_answer or\n",
    "                f\"Answer is C\" in generated_answer or\n",
    "                f\"Answer is D\" in generated_answer or\n",
    "                f\"answer is A\" in generated_answer or\n",
    "                f\"answer is B\" in generated_answer or\n",
    "                f\"answer is C\" in generated_answer or\n",
    "                f\"answer is D\" in generated_answer or\n",
    "                f\"Answer is: A\" in generated_answer or\n",
    "                f\"Answer is: B\" in generated_answer or\n",
    "                f\"Answer is: C\" in generated_answer or\n",
    "                f\"Answer is: D\" in generated_answer or\n",
    "                f\"answer is: A\" in generated_answer or\n",
    "                f\"answer is: B\" in generated_answer or\n",
    "                f\"answer is: C\" in generated_answer or\n",
    "                f\"answer is: D\" in generated_answer):\n",
    "            #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = 0\n",
    "                #\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = 0\n",
    "                #\n",
    "            else:\n",
    "                # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "                # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "                dataset_for_string_answer_evaluation.loc[dataset_for_string_answer_evaluation['id'] == dataset_record.id, \"string_answer_evaluation\"] = None\n",
    "                #\n",
    "        else:\n",
    "            print(f\"The string answer evaluation for id {dataset_record.id} was already generated\")\n",
    "    if extract_first_answer:\n",
    "        dataset_for_string_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_string_answer_evaluation_name}_string_answer_evaluated_extracted.csv'\n",
    "    else:\n",
    "        dataset_for_string_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_string_answer_evaluation_name}_string_answer_evaluated.csv'\n",
    "    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "    dataset_for_string_answer_evaluation.to_csv(dataset_for_string_answer_evaluation_address, index=False)\n",
    "    #\n",
    "    if extract_first_answer:\n",
    "        print(f\"The dataset for string answer evaluation was saved to {dataset_for_string_answer_evaluation_name}_string_answer_evaluated_extracted.csv\")\n",
    "    else:\n",
    "        print(f\"The dataset for string answer evaluation was saved to {dataset_for_string_answer_evaluation_name}_string_answer_evaluated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer_evaluate(data_for_tuning_folder_address, dataset_for_llm_answer_evaluation_name, dataset_for_llm_answer_evaluation, tot):\n",
    "    '''\n",
    "    This fuction evaluates generated answers for the correct answer based on the Large Language Model which could not be evaluated with pattern of string answers \n",
    "    where 1 is correct answer and 0 is incorrect answer\n",
    "    '''\n",
    "    # Code, that is, the answer generator, adapted from: LangChain, 2023. OllamaLLM [Online]. \n",
    "    # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "    answer_generator_llm = OllamaLLM(\n",
    "        # Code, that is, the parameter, reused from: LangChain, 2023. OllamaLLM [Online]. \n",
    "        # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "        # Code, that is, the value, reused from: Ollama, 2024. mixtral 8x7b-instruct-v0.1-fp16 [Online]. \n",
    "        # Available from: https://ollama.com/library/mixtral:8x7b-instruct-v0.1-fp16 [Accessed 25 September 2024].\n",
    "        model=\"mixtral:8x7b-instruct-v0.1-fp16\"\n",
    "        #\n",
    "        )\n",
    "    #\n",
    "    # Code, that is, the loop, adapted from: pandas, 2024. pandas.DataFrame.itertuples (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples [Accessed 1 September 2024].\n",
    "    for dataset_record in dataset_for_llm_answer_evaluation.itertuples():\n",
    "    #\n",
    "        if (dataset_record.string_answer_evaluation is None or\n",
    "        # Code, that is, the use of numpy that follows is, reused from: NumPy Developers, 2024. Constants. numpy.nan (v.2.1) [Online].\n",
    "        # Available from: https://numpy.org/doc/stable/reference/constants.html#numpy.nan [Accessed 15 September 2024]\n",
    "        dataset_record.string_answer_evaluation is np.nan or\n",
    "        #\n",
    "        # Code adapted from: pandas, 2024. pandas.isna (v.2.2) [Online]. \n",
    "        # Available from: https://pandas.pydata.org/docs/reference/api/pandas.isna.html#pandas-isna [Accessed 15 October 2024].\n",
    "        pd.isna(dataset_record.string_answer_evaluation) == True):\n",
    "        #\n",
    "            # Code is based on: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021a. \n",
    "            # Dataset Card for MMLU [Online]. s.l.: Hugging Face. Available from: https://huggingface.co/datasets/cais/mmlu [Accessed 5 August 2024].\n",
    "            if dataset_record.letter_answer == \"A\":\n",
    "                letter_answer_text = dataset_record.first_choice\n",
    "            elif dataset_record.letter_answer == \"B\":\n",
    "                letter_answer_text = dataset_record.second_choice\n",
    "            elif dataset_record.letter_answer == \"C\":\n",
    "                letter_answer_text = dataset_record.third_choice\n",
    "            elif dataset_record.letter_answer == \"D\":\n",
    "                letter_answer_text = dataset_record.fourth_choice\n",
    "            #\n",
    "\n",
    "            if tot:\n",
    "                prompt_to_evaluate_answer = prompt_tot_evaluation(dataset_record.enhanced_question, dataset_record.generated_answer, dataset_record.letter_answer, letter_answer_text)\n",
    "            else:\n",
    "                prompt_to_evaluate_answer = prompt_simple_evaluation(dataset_record.enhanced_question, dataset_record.generated_answer, dataset_record.letter_answer, letter_answer_text)\n",
    "\n",
    "            # Code, that is, the answer generation, adapted from: LangChain, 2023. OllamaLLM [Online]. \n",
    "            # Available from: https://python.langchain.com/v0.2/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html [Accessed 1 September 2024].\n",
    "            llm_checked_answer = answer_generator_llm.invoke(prompt_to_evaluate_answer)\n",
    "            #\n",
    "            print(llm_checked_answer)\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"llm_answer_check\"] = llm_checked_answer\n",
    "            #\n",
    "            if \"correct\" in llm_checked_answer.lower() and \"incorrect\" not in llm_checked_answer.lower() and \"not correct\" not in llm_checked_answer.lower():\n",
    "                llm_evaluated_answer = 1\n",
    "            else:\n",
    "                llm_evaluated_answer = 0\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"llm_answer_evaluation\"] = llm_evaluated_answer\n",
    "            #\n",
    "            # Code adapted from: pandas, 2024. pandas.DataFrame.loc (v.2.2) [Online]. \n",
    "            # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc [Accessed 5 September 2024].\n",
    "            dataset_for_llm_answer_evaluation.loc[dataset_for_llm_answer_evaluation['id'] == dataset_record.id, \"answer_evaluation\"] = llm_evaluated_answer\n",
    "            #\n",
    "        else:\n",
    "            print(f\"The string answer evaluation for id {dataset_record.id} was generated\")\n",
    "    dataset_for_llm_answer_evaluation_address = data_for_tuning_folder_address + '/' + f'{dataset_for_llm_answer_evaluation_name}_answer_evaluated.csv'\n",
    "    # Code, that is, the saving of the dataset, adapted from: pandas, 2024. pandas.DataFrame.to_csv (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv [Accessed 15 August 2024].\n",
    "    dataset_for_llm_answer_evaluation.to_csv(dataset_for_llm_answer_evaluation_address, index=False)\n",
    "    #\n",
    "    print(f\"The dataset for llm answer evaluation was saved to {dataset_for_llm_answer_evaluation_name}_answer_evaluated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataset_path_for_answer_evaluation, dataset_to_evaluate, column_to_evaluate):\n",
    "    '''\n",
    "    This function evaluates the accuracy of results in a particular column in a dataset\n",
    "    '''\n",
    "    # Code adapted from: pandas, 2024. pandas.DataFrame.sum (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html#pandas-dataframe-sum [Accessed 13 October 2024].\n",
    "    dataset_to_evaluate_column_ones = dataset_to_evaluate[f'{column_to_evaluate}'].sum()\n",
    "    #\n",
    "    # Code adapted from: pandas, 2024. pandas.DataFrame.count (v.2.2) [Online]. \n",
    "    # Available from: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.count.html#pandas-dataframe-count [Accessed 13 October 2024].\n",
    "    dataset_to_evaluate_column_numbers = dataset_to_evaluate[f'{column_to_evaluate}'].count()\n",
    "    #\n",
    "    # The equation is according to: Dutt, S., Chandramouli, S. and Das, A.K., 2019. Machine Learning [Online]. 1st ed. Uttar Pradesh, India: Pearson India. \n",
    "    # Available from: https://learning.oreilly.com/library/view/machine-learning-1st/9789353067373/ [Accessed 13 October 2024].\n",
    "    # Page 76\n",
    "    dataset_column_accuracy = dataset_to_evaluate_column_ones / dataset_to_evaluate_column_numbers\n",
    "    #\n",
    "    print(f'The dataset to evaluate column ones of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_to_evaluate_column_ones}')\n",
    "    print(f'The dataset to evaluate column numbers of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_to_evaluate_column_numbers}')\n",
    "    print(f'The dataset column accuracy of {column_to_evaluate} in {dataset_path_for_answer_evaluation} is {dataset_column_accuracy}')\n",
    "    return dataset_column_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral with simple prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first answer in the string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = False\n",
    "extract_first_answer = True\n",
    "test_data_for_evaluation_folder_name = \"test-dataset-for-evaluation-mistral-simple\"\n",
    "test_data_for_evaluation_folder_address = create_data_for_tuning_folder_address(current_path, test_data_for_evaluation_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_for_data_for_tuning(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows = check_and_load_generated_answers_to_continue(test_dataset, test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "generate_and_check_answers(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address, test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not extract first answer in string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_first_answer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral with ToT prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first answer in string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = True\n",
    "extract_first_answer = True\n",
    "test_data_for_evaluation_folder_name = \"test-dataset-for-evaluation-mistral-tot\"\n",
    "test_data_for_evaluation_folder_address = create_data_for_tuning_folder_address(current_path, test_data_for_evaluation_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_for_data_for_tuning(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows = check_and_load_generated_answers_to_continue(test_dataset, test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address)\n",
    "generate_and_check_answers(test_data_for_evaluation_folder_name, test_data_for_evaluation_folder_address, test_dataset_for_generated_answers, test_dataset_for_generated_answers_version, test_generated_answers_count, test_total_dataset_rows, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_extracted_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not extract first answer in string answer evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_first_answer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_string_answer_evaluation_name = \"test_dataset_45_671_671\"\n",
    "test_dataset_path_for_string_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_string_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for string answer evaluation csv is {test_dataset_path_for_string_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_string_answer_evaluation = pd.read_csv(test_dataset_path_for_string_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "string_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_string_answer_evaluation_name, test_dataset_for_string_answer_evaluation, extract_first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_llm_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated\"\n",
    "test_dataset_path_for_llm_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_llm_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for llm answer evaluation csv is {test_dataset_path_for_llm_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_llm_answer_evaluation = pd.read_csv(test_dataset_path_for_llm_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "llm_answer_evaluate(test_data_for_evaluation_folder_address, test_dataset_for_llm_answer_evaluation_name, test_dataset_for_llm_answer_evaluation, tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_for_answer_evaluation_name = \"test_dataset_45_671_671_string_answer_evaluated_answer_evaluated\"\n",
    "test_dataset_path_for_answer_evaluation_csv = f\"{test_data_for_evaluation_folder_name}/{test_dataset_for_answer_evaluation_name}.csv\"\n",
    "print(f'Test dataset path for answer evaluation csv is {test_dataset_path_for_answer_evaluation_csv}')\n",
    "\n",
    "# Code, that is, the loading of the dataset, adapted from: pandas, 2024. pandas.read_csv (v.2.2) [Online]. \n",
    "# Available from: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html [Accessed 17 August 2024].\n",
    "test_dataset_for_answer_evaluation = pd.read_csv(test_dataset_path_for_answer_evaluation_csv)\n",
    "#\n",
    "\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"string_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"llm_answer_evaluation\")\n",
    "evaluate_accuracy(test_dataset_path_for_answer_evaluation_csv, test_dataset_for_answer_evaluation, \"answer_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
